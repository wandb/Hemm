{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hemm: Holistic Evaluation of Multi-modal Generative Models","text":"<p>Hemm is a library for performing comprehensive benchmark of text-to-image diffusion models on image quality and prompt comprehension integrated with Weights &amp; Biases and Weave. </p> <p>Hemm is highly inspired by the following projects:</p> <ul> <li> <p>Holistic Evaluation of Text-To-Image Models</p> </li> <li> <p>T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation</p> </li> <li> <p>T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation</p> </li> <li> <p>GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment</p> </li> </ul> The evaluation pipeline will take each example, pass it through your application and score the output on multiple custom scoring functions using Weave Evaluation. By doing this, you'll have a view of the performance of your model, and a rich UI to drill into individual ouputs and scores."},{"location":"#leaderboards","title":"Leaderboards","text":"Leaderboard Weave Evals Rendering prompts with Complex Actions Weave Evals"},{"location":"#installation","title":"Installation","text":"<p>First, we recommend you install the PyTorch by visiting pytorch.org/get-started/locally.</p> <pre><code>git clone https://github.com/wandb/Hemm\ncd Hemm\npip install -e \".[core]\"\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>First, you need to publish your evaluation dataset to Weave. Check out this tutorial that shows you how to publish a dataset on your project.</p> <p>Once you have a dataset on your Weave project, you can evaluate a text-to-image generation model on the metrics.</p> <pre><code>import wandb\nimport weave\n\n\nfrom hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\nfrom hemm.metrics.prompt_alignment import CLIPImageQualityScoreMetric, CLIPScoreMetric\n\n\n# Initialize Weave and WandB\nwandb.init(project=\"image-quality-leaderboard\", job_type=\"evaluation\")\nweave.init(project_name=\"image-quality-leaderboard\")\n\n\n# Initialize the diffusion model to be evaluated as a `weave.Model` using `BaseWeaveModel`\n# The `BaseDiffusionModel` class uses a `diffusers.DiffusionPipeline` under the hood.\n# You can write your own model `weave.Model` if your model is not diffusers compatible.\nmodel = BaseDiffusionModel(diffusion_model_name_or_path=\"CompVis/stable-diffusion-v1-4\")\n\n\n# Add the model to the evaluation pipeline\nevaluation_pipeline = EvaluationPipeline(model=model)\n\n\n# Add PSNR Metric to the evaluation pipeline\npsnr_metric = PSNRMetric(image_size=evaluation_pipeline.image_size)\nevaluation_pipeline.add_metric(psnr_metric)\n\n\n# Add SSIM Metric to the evaluation pipeline\nssim_metric = SSIMMetric(image_size=evaluation_pipeline.image_size)\nevaluation_pipeline.add_metric(ssim_metric)\n\n\n# Add LPIPS Metric to the evaluation pipeline\nlpips_metric = LPIPSMetric(image_size=evaluation_pipeline.image_size)\nevaluation_pipeline.add_metric(lpips_metric)\n\n\n# Get the Weave dataset reference\ndataset = weave.ref(\"COCO:v0\").get()\n\n\n# Evaluate!\nevaluation_pipeline(dataset=dataset)\n</code></pre>"},{"location":"eval_pipelines/","title":"Evaluation Pipelines","text":"<p>Hemm evaluation pipelines for Diffusers pipelines.</p>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline","title":"<code>EvaluationPipeline</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Evaluation pipeline to evaluate the a multi-modal generative model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseDiffusionModel</code> <p>The model to evaluate.</p> required <code>seed</code> <code>int</code> <p>Seed value for the random number generator.</p> <code>42</code> <code>mock_inference_dataset_address</code> <code>Optional[str]</code> <p>A wandb dataset artifact address which if provided will mock inference results. This prevents the need for redundant generations when switching metrics/judges with the same evaluation datset(s).</p> <code>None</code> <code>save_inference_dataset_name</code> <code>Optional[str]</code> <p>A weave dataset name which if provided will save inference results as a separate weave dataset.</p> <code>None</code> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>class EvaluationPipeline(ABC):\n    \"\"\"Evaluation pipeline to evaluate the a multi-modal generative model.\n\n    Args:\n        model (BaseDiffusionModel): The model to evaluate.\n        seed (int): Seed value for the random number generator.\n        mock_inference_dataset_address (Optional[str]): A wandb dataset artifact address which if\n            provided will mock inference results. This prevents the need for redundant generations\n            when switching metrics/judges with the same evaluation datset(s).\n        save_inference_dataset_name (Optional[str]): A weave dataset name which if provided will\n            save inference results as a separate weave dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: MODEL_TYPE,\n        seed: int = 42,\n        mock_inference_dataset_address: Optional[str] = None,\n        save_inference_dataset_name: Optional[str] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.model = model\n\n        self.image_size = (self.model.image_height, self.model.image_width)\n        self.seed = seed\n        self.mock_inference_dataset_address = mock_inference_dataset_address\n        if mock_inference_dataset_address:\n            self.save_inference_dataset_name = None\n            artifact = wandb.use_artifact(\n                self.mock_inference_dataset_address, type=\"dataset\"\n            )\n            self.mock_inference_dataset_dir = artifact.download()\n\n        else:\n            self.save_inference_dataset_name = save_inference_dataset_name\n\n        if self.save_inference_dataset_name:\n            os.makedirs(\n                os.path.join(\"inference_dataset\", self.save_inference_dataset_name),\n                exist_ok=True,\n            )\n\n        self.inference_counter = 0\n        self.table_columns = [\"model\", \"prompt\", \"generated_image\"]\n        self.table_rows: List = []\n        self.evaluation_table: wandb.Table = None\n        self.metric_functions: List[BaseMetric] = []\n\n        self.evaluation_configs = {\n            \"pretrained_model_name_or_path\": self.model.diffusion_model_name_or_path,\n            \"torch_dtype\": str(self.model._torch_dtype),\n            \"enable_cpu_offfload\": self.model.enable_cpu_offfload,\n            \"image_size\": {\n                \"height\": self.image_size[0],\n                \"width\": self.image_size[1],\n            },\n            \"seed\": seed,\n            \"diffusion_pipeline\": dict(self.model._pipeline.config),\n        }\n\n    def add_metric(self, metric_fn: BaseMetric):\n        \"\"\"Add a metric function to the evaluation pipeline.\n\n        Args:\n            metric_fn (BaseMetric): Metric function to evaluate the generated images.\n        \"\"\"\n        self.table_columns.append(metric_fn.__class__.__name__)\n        self.evaluation_configs.update(metric_fn.config)\n        self.metric_functions.append(metric_fn)\n\n    @weave.op()\n    def infer(self, prompt: str) -&gt; Dict[str, str]:\n        \"\"\"Inference function to generate images for the given prompt.\n\n        Args:\n            prompt (str): Prompt to generate the image.\n\n        Returns:\n            Dict[str, str]: Dictionary containing base64 encoded image to be logged as\n                a Weave object.\n        \"\"\"\n        if self.inference_counter == 0:\n            self.evaluation_table = wandb.Table(columns=self.table_columns)\n        if self.mock_inference_dataset_address:\n            image = Image.open(\n                os.path.join(\n                    self.mock_inference_dataset_dir, f\"{self.inference_counter}.png\"\n                )\n            )\n            output = {\"image\": image}\n        else:\n            output = self.model.predict(prompt, seed=self.seed)\n        self.table_rows.append(\n            [self.model.diffusion_model_name_or_path, prompt, output[\"image\"]]\n        )\n        if self.save_inference_dataset_name:\n            output[\"image\"].save(\n                os.path.join(\n                    \"inference_dataset\",\n                    self.save_inference_dataset_name,\n                    f\"{self.inference_counter}.png\",\n                )\n            )\n        self.inference_counter += 1\n        return output\n\n    @weave.op()\n    async def infer_async(self, prompt: str) -&gt; Dict[str, str]:\n        \"\"\"Async inference function to generate images for the given prompt.\n\n        Args:\n            prompt (str): Prompt to generate the image.\n\n        Returns:\n            Dict[str, str]: Dictionary containing base64 encoded image to be logged as\n                a Weave object.\n        \"\"\"\n        return self.infer(prompt)\n\n    def log_summary(self, summary: Dict[str, float]) -&gt; None:\n        \"\"\"Log the evaluation summary to the Weights &amp; Biases dashboard.\"\"\"\n        config = wandb.config\n        config.update(self.evaluation_configs)\n        for row_idx, row in enumerate(self.table_rows):\n            current_row = row\n            current_row[-1] = wandb.Image(current_row[-1])\n            for metric_fn in self.metric_functions:\n                current_row.append(metric_fn.scores[row_idx])\n            self.evaluation_table.add_data(*current_row)\n        summary_table = wandb.Table(columns=[\"summary\"], data=[[summary]])\n        wandb.log(\n            {\n                \"evalution\": self.evaluation_table,\n                \"summary\": summary_table,\n                \"evaluation_summary\": summary,\n            }\n        )\n\n    def save_inference_results(self):\n        artifact = wandb.Artifact(name=self.save_inference_dataset_name, type=\"dataset\")\n        artifact.add_dir(\n            os.path.join(\"inference_dataset\", self.save_inference_dataset_name)\n        )\n        artifact.save()\n\n    def cleanup(self):\n        \"\"\"Cleanup the inference dataset directory. Should be called after the evaluation is complete\n        and `wandb.finish()` is called.\"\"\"\n        if os.path.exists(\"inference_dataset\"):\n            shutil.rmtree(\"inference_dataset\")\n\n    def __call__(\n        self, dataset: Union[List[Dict], str], async_infer: bool = False\n    ) -&gt; Dict[str, float]:\n        \"\"\"Evaluate the Stable Diffusion model on the given dataset.\n\n        Args:\n            dataset (Union[List[Dict], str]): Dataset to evaluate the model on. If a string is\n                passed, it is assumed to be a Weave dataset reference.\n            async_infer (bool, optional): Whether to use async inference. Defaults to False.\n        \"\"\"\n        dataset = weave.ref(dataset).get() if isinstance(dataset, str) else dataset\n        evaluation = weave.Evaluation(\n            dataset=dataset,\n            scorers=[\n                metric_fn.evaluate_async if async_infer else metric_fn.evaluate\n                for metric_fn in self.metric_functions\n            ],\n        )\n        self.model.configs.update(self.evaluation_configs)\n        summary = asyncio.run(evaluation.evaluate(self.infer_async))\n        self.log_summary(summary)\n        if self.save_inference_dataset_name:\n            self.save_inference_results()\n        return summary\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.__call__","title":"<code>__call__(dataset, async_infer=False)</code>","text":"<p>Evaluate the Stable Diffusion model on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[List[Dict], str]</code> <p>Dataset to evaluate the model on. If a string is passed, it is assumed to be a Weave dataset reference.</p> required <code>async_infer</code> <code>bool</code> <p>Whether to use async inference. Defaults to False.</p> <code>False</code> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def __call__(\n    self, dataset: Union[List[Dict], str], async_infer: bool = False\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the Stable Diffusion model on the given dataset.\n\n    Args:\n        dataset (Union[List[Dict], str]): Dataset to evaluate the model on. If a string is\n            passed, it is assumed to be a Weave dataset reference.\n        async_infer (bool, optional): Whether to use async inference. Defaults to False.\n    \"\"\"\n    dataset = weave.ref(dataset).get() if isinstance(dataset, str) else dataset\n    evaluation = weave.Evaluation(\n        dataset=dataset,\n        scorers=[\n            metric_fn.evaluate_async if async_infer else metric_fn.evaluate\n            for metric_fn in self.metric_functions\n        ],\n    )\n    self.model.configs.update(self.evaluation_configs)\n    summary = asyncio.run(evaluation.evaluate(self.infer_async))\n    self.log_summary(summary)\n    if self.save_inference_dataset_name:\n        self.save_inference_results()\n    return summary\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.add_metric","title":"<code>add_metric(metric_fn)</code>","text":"<p>Add a metric function to the evaluation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>metric_fn</code> <code>BaseMetric</code> <p>Metric function to evaluate the generated images.</p> required Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def add_metric(self, metric_fn: BaseMetric):\n    \"\"\"Add a metric function to the evaluation pipeline.\n\n    Args:\n        metric_fn (BaseMetric): Metric function to evaluate the generated images.\n    \"\"\"\n    self.table_columns.append(metric_fn.__class__.__name__)\n    self.evaluation_configs.update(metric_fn.config)\n    self.metric_functions.append(metric_fn)\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup the inference dataset directory. Should be called after the evaluation is complete and <code>wandb.finish()</code> is called.</p> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def cleanup(self):\n    \"\"\"Cleanup the inference dataset directory. Should be called after the evaluation is complete\n    and `wandb.finish()` is called.\"\"\"\n    if os.path.exists(\"inference_dataset\"):\n        shutil.rmtree(\"inference_dataset\")\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.infer","title":"<code>infer(prompt)</code>","text":"<p>Inference function to generate images for the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to generate the image.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dictionary containing base64 encoded image to be logged as a Weave object.</p> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>@weave.op()\ndef infer(self, prompt: str) -&gt; Dict[str, str]:\n    \"\"\"Inference function to generate images for the given prompt.\n\n    Args:\n        prompt (str): Prompt to generate the image.\n\n    Returns:\n        Dict[str, str]: Dictionary containing base64 encoded image to be logged as\n            a Weave object.\n    \"\"\"\n    if self.inference_counter == 0:\n        self.evaluation_table = wandb.Table(columns=self.table_columns)\n    if self.mock_inference_dataset_address:\n        image = Image.open(\n            os.path.join(\n                self.mock_inference_dataset_dir, f\"{self.inference_counter}.png\"\n            )\n        )\n        output = {\"image\": image}\n    else:\n        output = self.model.predict(prompt, seed=self.seed)\n    self.table_rows.append(\n        [self.model.diffusion_model_name_or_path, prompt, output[\"image\"]]\n    )\n    if self.save_inference_dataset_name:\n        output[\"image\"].save(\n            os.path.join(\n                \"inference_dataset\",\n                self.save_inference_dataset_name,\n                f\"{self.inference_counter}.png\",\n            )\n        )\n    self.inference_counter += 1\n    return output\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.infer_async","title":"<code>infer_async(prompt)</code>  <code>async</code>","text":"<p>Async inference function to generate images for the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to generate the image.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dictionary containing base64 encoded image to be logged as a Weave object.</p> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>@weave.op()\nasync def infer_async(self, prompt: str) -&gt; Dict[str, str]:\n    \"\"\"Async inference function to generate images for the given prompt.\n\n    Args:\n        prompt (str): Prompt to generate the image.\n\n    Returns:\n        Dict[str, str]: Dictionary containing base64 encoded image to be logged as\n            a Weave object.\n    \"\"\"\n    return self.infer(prompt)\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.log_summary","title":"<code>log_summary(summary)</code>","text":"<p>Log the evaluation summary to the Weights &amp; Biases dashboard.</p> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def log_summary(self, summary: Dict[str, float]) -&gt; None:\n    \"\"\"Log the evaluation summary to the Weights &amp; Biases dashboard.\"\"\"\n    config = wandb.config\n    config.update(self.evaluation_configs)\n    for row_idx, row in enumerate(self.table_rows):\n        current_row = row\n        current_row[-1] = wandb.Image(current_row[-1])\n        for metric_fn in self.metric_functions:\n            current_row.append(metric_fn.scores[row_idx])\n        self.evaluation_table.add_data(*current_row)\n    summary_table = wandb.Table(columns=[\"summary\"], data=[[summary]])\n    wandb.log(\n        {\n            \"evalution\": self.evaluation_table,\n            \"summary\": summary_table,\n            \"evaluation_summary\": summary,\n        }\n    )\n</code></pre>"},{"location":"utils/","title":"Hemm utilities","text":""},{"location":"utils/#hemm.utils.base64_decode_image","title":"<code>base64_decode_image(image)</code>","text":"<p>Decodes a base64 encoded image string encoded using the function <code>hemm.utils.base64_encode_image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image string encoded using the function <code>hemm.utils.base64_encode_image</code>.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: PIL Image object.</p> Source code in <code>hemm/utils.py</code> <pre><code>def base64_decode_image(image: str) -&gt; Image.Image:\n    \"\"\"Decodes a base64 encoded image string encoded using the function `hemm.utils.base64_encode_image`.\n\n    Args:\n        image (str): Base64 encoded image string encoded using the function `hemm.utils.base64_encode_image`.\n\n    Returns:\n        Image.Image: PIL Image object.\n    \"\"\"\n    return Image.open(io.BytesIO(base64.b64decode(image.split(\";base64,\")[-1])))\n</code></pre>"},{"location":"utils/#hemm.utils.base64_encode_image","title":"<code>base64_encode_image(image_path, mimetype=None)</code>","text":"<p>Converts an image to base64 encoded string to be logged and rendered on Weave dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Image]</code> <p>Path to the image or PIL Image object.</p> required <code>mimetype</code> <code>Optional[str]</code> <p>Mimetype of the image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Base64 encoded image string.</p> Source code in <code>hemm/utils.py</code> <pre><code>def base64_encode_image(\n    image_path: Union[str, Image.Image], mimetype: Optional[str] = None\n) -&gt; str:\n    \"\"\"Converts an image to base64 encoded string to be logged and rendered on Weave dashboard.\n\n    Args:\n        image_path (Union[str, Image.Image]): Path to the image or PIL Image object.\n        mimetype (Optional[str], optional): Mimetype of the image. Defaults to None.\n\n    Returns:\n        str: Base64 encoded image string.\n    \"\"\"\n    image = Image.open(image_path) if isinstance(image_path, str) else image_path\n    mimetype = (\n        EXT_TO_MIMETYPE[Path(image_path).suffix]\n        if isinstance(image_path, str)\n        else \"image/png\"\n    )\n    byte_arr = io.BytesIO()\n    image.save(byte_arr, format=\"PNG\")\n    encoded_string = base64.b64encode(byte_arr.getvalue()).decode(\"utf-8\")\n    encoded_string = f\"data:{mimetype};base64,{encoded_string}\"\n    return str(encoded_string)\n</code></pre>"},{"location":"utils/#hemm.utils.publish_dataset_to_weave","title":"<code>publish_dataset_to_weave(dataset_path, dataset_name=None, prompt_column=None, ground_truth_image_column=None, split=None, data_limit=None, get_weave_dataset_reference=True, dataset_transforms=None, dump_dir='./dump', *args, **kwargs)</code>","text":"<p>Publishes a HuggingFace dataset dictionary dataset as a Weave dataset.</p> Publish a subset of MSCOCO from Huggingface as a Weave Dataset <pre><code>import weave\nfrom hemm.utils import publish_dataset_to_weave\n\nif __name__ == \"__main__\":\n    weave.init(project_name=\"t2i_eval\")\n\n    def preprocess_sentences_column(example):\n        example[\"sentences\"] = example[\"sentences\"][\"raw\"]\n        return example\n\n\n    dataset_reference = publish_dataset_to_weave(\n        dataset_path=\"HuggingFaceM4/COCO\",\n        prompt_column=\"sentences\",\n        ground_truth_image_column=\"image\",\n        split=\"validation\",\n        dataset_transforms=preprocess_sentences_column,\n        data_limit=10,\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>[type]</code> <p>Path to the HuggingFace dataset.</p> required <code>dataset_name</code> <code>Optional[str]</code> <p>Name of the Weave dataset.</p> <code>None</code> <code>prompt_column</code> <code>Optional[str]</code> <p>Column name for prompt.</p> <code>None</code> <code>ground_truth_image_column</code> <code>Optional[str]</code> <p>Column name for ground truth image.</p> <code>None</code> <code>split</code> <code>Optional[str]</code> <p>Split to be used.</p> <code>None</code> <code>data_limit</code> <code>Optional[int]</code> <p>Limit the number of data items.</p> <code>None</code> <code>get_weave_dataset_reference</code> <code>bool</code> <p>Whether to return the Weave dataset reference.</p> <code>True</code> <code>dataset_transforms</code> <code>Optional[List[Callable]]</code> <p>List of dataset transforms.</p> <code>None</code> <code>dump_dir</code> <code>Optional[str]</code> <p>Directory to dump the results.</p> <code>'./dump'</code> <p>Returns:</p> Type Description <code>Union[ObjectRef, None]</code> <p>Union[ObjectRef, None]: Weave dataset reference if get_weave_dataset_reference is True.</p> Source code in <code>hemm/utils.py</code> <pre><code>def publish_dataset_to_weave(\n    dataset_path,\n    dataset_name: Optional[str] = None,\n    prompt_column: Optional[str] = None,\n    ground_truth_image_column: Optional[str] = None,\n    split: Optional[str] = None,\n    data_limit: Optional[int] = None,\n    get_weave_dataset_reference: bool = True,\n    dataset_transforms: Optional[List[Callable]] = None,\n    dump_dir: Optional[str] = \"./dump\",\n    *args,\n    **kwargs,\n) -&gt; Union[ObjectRef, None]:\n    \"\"\"Publishes a HuggingFace dataset dictionary dataset as a Weave dataset.\n\n    ??? example \"Publish a subset of MSCOCO from Huggingface as a Weave Dataset\"\n        ```python\n        import weave\n        from hemm.utils import publish_dataset_to_weave\n\n        if __name__ == \"__main__\":\n            weave.init(project_name=\"t2i_eval\")\n\n            def preprocess_sentences_column(example):\n                example[\"sentences\"] = example[\"sentences\"][\"raw\"]\n                return example\n\n\n            dataset_reference = publish_dataset_to_weave(\n                dataset_path=\"HuggingFaceM4/COCO\",\n                prompt_column=\"sentences\",\n                ground_truth_image_column=\"image\",\n                split=\"validation\",\n                dataset_transforms=preprocess_sentences_column,\n                data_limit=10,\n            )\n        ```\n\n    Args:\n        dataset_path ([type]): Path to the HuggingFace dataset.\n        dataset_name (Optional[str], optional): Name of the Weave dataset.\n        prompt_column (Optional[str], optional): Column name for prompt.\n        ground_truth_image_column (Optional[str], optional): Column name for ground truth image.\n        split (Optional[str], optional): Split to be used.\n        data_limit (Optional[int], optional): Limit the number of data items.\n        get_weave_dataset_reference (bool, optional): Whether to return the Weave dataset reference.\n        dataset_transforms (Optional[List[Callable]], optional): List of dataset transforms.\n        dump_dir (Optional[str], optional): Directory to dump the results.\n\n    Returns:\n        Union[ObjectRef, None]: Weave dataset reference if get_weave_dataset_reference is True.\n    \"\"\"\n    os.makedirs(dump_dir, exist_ok=True)\n    dataset_name = dataset_name or Path(dataset_path).stem\n    dataset_dict = load_dataset(dataset_path, *args, **kwargs)\n    dataset_dict = dataset_dict[split] if split else dataset_dict[\"train\"]\n    dataset_dict = (\n        dataset_dict.select(range(data_limit))\n        if data_limit is not None and data_limit &lt; len(dataset_dict)\n        else dataset_dict\n    )\n    if dataset_transforms:\n        for transform in dataset_transforms:\n            dataset_dict = dataset_dict.map(transform)\n    dataset_dict = (\n        dataset_dict.rename_column(prompt_column, \"prompt\")\n        if prompt_column\n        else dataset_dict\n    )\n    dataset_dict = (\n        dataset_dict.rename_column(ground_truth_image_column, \"ground_truth_image\")\n        if ground_truth_image_column\n        else dataset_dict\n    )\n    weave_dataset_rows = [data_item for data_item in tqdm(dataset_dict)]\n\n    weave_dataset = weave.Dataset(name=dataset_name, rows=weave_dataset_rows)\n    weave.publish(weave_dataset)\n    return weave.ref(dataset_name).get() if get_weave_dataset_reference else None\n</code></pre>"},{"location":"metrics/image_quality/","title":"Image Quality Metrics","text":""},{"location":"metrics/image_quality/#hemm.metrics.image_quality.LPIPSMetric","title":"<code>LPIPSMetric</code>","text":"<p>               Bases: <code>BaseImageQualityMetric</code></p> <p>LPIPS Metric to compute the Learned Perceptual Image Patch Similarity (LPIPS) score between two images. LPIPS essentially computes the similarity between the activations of two image patches for some pre-defined network. This measure has been shown to match human perception well. A low LPIPS score means that image patches are perceptual similar.</p> <p>Parameters:</p> Name Type Description Default <code>lpips_net_type</code> <code>str</code> <p>The network type to use for computing LPIPS. One of \"alex\", \"vgg\", or \"squeeze\".</p> <code>'alex'</code> <code>image_size</code> <code>Tuple[int, int]</code> <p>The size to which images will be resized before computing LPIPS.</p> <code>(512, 512)</code> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'alexnet_learned_perceptual_image_patch_similarity'</code> Source code in <code>hemm/metrics/image_quality/lpips.py</code> <pre><code>class LPIPSMetric(BaseImageQualityMetric):\n    \"\"\"LPIPS Metric to compute the Learned Perceptual Image Patch Similarity (LPIPS) score\n    between two images. LPIPS essentially computes the similarity between the activations of\n    two image patches for some pre-defined network. This measure has been shown to match\n    human perception well. A low LPIPS score means that image patches are perceptual similar.\n\n    Args:\n        lpips_net_type (str): The network type to use for computing LPIPS. One of \"alex\", \"vgg\",\n            or \"squeeze\".\n        image_size (Tuple[int, int]): The size to which images will be resized before computing\n            LPIPS.\n        name (str): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        lpips_net_type: Literal[\"alex\", \"vgg\", \"squeeze\"] = \"alex\",\n        image_size: Optional[Tuple[int, int]] = (512, 512),\n        name: str = \"alexnet_learned_perceptual_image_patch_similarity\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.image_size = image_size\n        self.lpips_metric = partial(\n            learned_perceptual_image_patch_similarity, net_type=lpips_net_type\n        )\n        self.config = {\"lpips_net_type\": lpips_net_type}\n\n    @weave.op()\n    def compute_metric(\n        self, ground_truth_pil_image: Image, generated_pil_image: Image, prompt: str\n    ) -&gt; ComputeMetricOutput:\n        ground_truth_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(ground_truth_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 2, 1)\n            .float()\n        )\n        generated_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(generated_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 2, 1)\n            .float()\n        )\n        ground_truth_image = (ground_truth_image / 127.5) - 1.0\n        generated_image = (generated_image / 127.5) - 1.0\n        return ComputeMetricOutput(\n            score=float(\n                self.lpips_metric(generated_image, ground_truth_image).detach()\n            ),\n            ground_truth_image=base64_encode_image(ground_truth_pil_image),\n        )\n\n    @weave.op()\n    def evaluate(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"LPIPSMetric\"\n        return super().evaluate(prompt, ground_truth_image, model_output)\n\n    @weave.op()\n    async def evaluate_async(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"LPIPSMetric\"\n        return self.evaluate(prompt, ground_truth_image, model_output)\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.PSNRMetric","title":"<code>PSNRMetric</code>","text":"<p>               Bases: <code>BaseImageQualityMetric</code></p> <p>PSNR Metric to compute the Peak Signal-to-Noise Ratio (PSNR) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>psnr_data_range</code> <code>Optional[Union[float, Tuple[float, float]]]</code> <p>The data range of the input image (min, max). If None, the data range is determined from the image data type.</p> <code>None</code> <code>psnr_base</code> <code>float</code> <p>The base of the logarithm in the PSNR formula.</p> <code>10.0</code> <code>image_size</code> <code>Tuple[int, int]</code> <p>The size to which images will be resized before computing PSNR.</p> <code>(512, 512)</code> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'peak_signal_noise_ratio'</code> Source code in <code>hemm/metrics/image_quality/psnr.py</code> <pre><code>class PSNRMetric(BaseImageQualityMetric):\n    \"\"\"PSNR Metric to compute the Peak Signal-to-Noise Ratio (PSNR) between two images.\n\n    Args:\n        psnr_data_range (Optional[Union[float, Tuple[float, float]]]): The data range of the input\n            image (min, max). If None, the data range is determined from the image data type.\n        psnr_base (float): The base of the logarithm in the PSNR formula.\n        image_size (Tuple[int, int]): The size to which images will be resized before computing\n            PSNR.\n        name (str): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        psnr_data_range: Optional[Union[float, Tuple[float, float]]] = None,\n        psnr_base: float = 10.0,\n        image_size: Optional[Tuple[int, int]] = (512, 512),\n        name: str = \"peak_signal_noise_ratio\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.image_size = image_size\n        self.psnr_metric = partial(\n            peak_signal_noise_ratio, data_range=psnr_data_range, base=psnr_base\n        )\n        self.config = {\n            \"psnr_base\": psnr_base,\n            \"psnr_data_range\": psnr_data_range,\n            \"image_size\": image_size,\n        }\n\n    @weave.op()\n    def compute_metric(\n        self,\n        ground_truth_pil_image: Image.Image,\n        generated_pil_image: Image.Image,\n        prompt: str,\n    ) -&gt; ComputeMetricOutput:\n        ground_truth_image = torch.from_numpy(\n            np.expand_dims(\n                np.array(ground_truth_pil_image.resize(self.image_size)), axis=0\n            ).astype(np.uint8)\n        ).float()\n        generated_image = torch.from_numpy(\n            np.expand_dims(\n                np.array(generated_pil_image.resize(self.image_size)), axis=0\n            ).astype(np.uint8)\n        ).float()\n        return ComputeMetricOutput(\n            score=float(self.psnr_metric(generated_image, ground_truth_image).detach()),\n            ground_truth_image=base64_encode_image(ground_truth_pil_image),\n        )\n\n    @weave.op()\n    def evaluate(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"PSNRMetric\"\n        return super().evaluate(prompt, ground_truth_image, model_output)\n\n    @weave.op()\n    async def evaluate_async(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"PSNRMetric\"\n        return self.evaluate(prompt, ground_truth_image, model_output)\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.SSIMMetric","title":"<code>SSIMMetric</code>","text":"<p>               Bases: <code>BaseImageQualityMetric</code></p> <p>SSIM Metric to compute the Structural Similarity Index Measure (SSIM) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>ssim_gaussian_kernel</code> <code>bool</code> <p>Whether to use a Gaussian kernel for SSIM computation.</p> <code>True</code> <code>ssim_sigma</code> <code>float</code> <p>The standard deviation of the Gaussian kernel.</p> <code>1.5</code> <code>ssim_kernel_size</code> <code>int</code> <p>The size of the Gaussian kernel.</p> <code>11</code> <code>ssim_data_range</code> <code>Optional[Union[float, Tuple[float, float]]]</code> <p>The data range of the input image (min, max). If None, the data range is determined from the image data type.</p> <code>None</code> <code>ssim_k1</code> <code>float</code> <p>The constant used to stabilize the SSIM numerator.</p> <code>0.01</code> <code>ssim_k2</code> <code>float</code> <p>The constant used to stabilize the SSIM denominator.</p> <code>0.03</code> <code>image_size</code> <code>Tuple[int, int]</code> <p>The size to which images will be resized before computing SSIM.</p> <code>(512, 512)</code> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'structural_similarity_index_measure'</code> Source code in <code>hemm/metrics/image_quality/ssim.py</code> <pre><code>class SSIMMetric(BaseImageQualityMetric):\n    \"\"\"SSIM Metric to compute the\n    [Structural Similarity Index Measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity)\n    between two images.\n\n    Args:\n        ssim_gaussian_kernel (bool): Whether to use a Gaussian kernel for SSIM computation.\n        ssim_sigma (float): The standard deviation of the Gaussian kernel.\n        ssim_kernel_size (int): The size of the Gaussian kernel.\n        ssim_data_range (Optional[Union[float, Tuple[float, float]]]): The data range of the input\n            image (min, max). If None, the data range is determined from the image data type.\n        ssim_k1 (float): The constant used to stabilize the SSIM numerator.\n        ssim_k2 (float): The constant used to stabilize the SSIM denominator.\n        image_size (Tuple[int, int]): The size to which images will be resized before computing\n            SSIM.\n        name (str): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        ssim_gaussian_kernel: bool = True,\n        ssim_sigma: float = 1.5,\n        ssim_kernel_size: int = 11,\n        ssim_data_range: Union[float, Tuple[float, float], None] = None,\n        ssim_k1: float = 0.01,\n        ssim_k2: float = 0.03,\n        image_size: Optional[Tuple[int, int]] = (512, 512),\n        name: str = \"structural_similarity_index_measure\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.image_size = image_size\n        self.ssim_metric = partial(\n            structural_similarity_index_measure,\n            gaussian_kernel=ssim_gaussian_kernel,\n            sigma=ssim_sigma,\n            kernel_size=ssim_kernel_size,\n            data_range=ssim_data_range,\n            k1=ssim_k1,\n            k2=ssim_k2,\n        )\n        self.config = {\n            \"ssim_gaussian_kernel\": ssim_gaussian_kernel,\n            \"ssim_sigma\": ssim_sigma,\n            \"ssim_kernel_size\": ssim_kernel_size,\n            \"ssim_data_range\": ssim_data_range,\n            \"ssim_k1\": ssim_k1,\n            \"ssim_k2\": ssim_k2,\n        }\n\n    @weave.op()\n    def compute_metric(\n        self,\n        ground_truth_pil_image: Image.Image,\n        generated_pil_image: Image.Image,\n        prompt: str,\n    ) -&gt; ComputeMetricOutput:\n        ground_truth_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(ground_truth_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 1, 2)\n            .float()\n        )\n        generated_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(generated_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 1, 2)\n            .float()\n        )\n        return ComputeMetricOutput(\n            score=float(self.ssim_metric(generated_image, ground_truth_image)),\n            ground_truth_image=base64_encode_image(ground_truth_pil_image),\n        )\n\n    @weave.op()\n    def evaluate(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"SSIMMetric\"\n        return super().evaluate(prompt, ground_truth_image, model_output)\n\n    @weave.op()\n    async def evaluate_async(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"SSIMMetric\"\n        return self.evaluate(prompt, ground_truth_image, model_output)\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric","title":"<code>BaseImageQualityMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>class BaseImageQualityMetric(BaseMetric):\n\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Base class for Image Quality Metrics.\n\n        Args:\n            name (str): Name of the metric.\n        \"\"\"\n        super().__init__()\n        self.scores = []\n        self.name = name\n        self.config = {}\n\n    @abstractmethod\n    def compute_metric(\n        self,\n        ground_truth_pil_image: Image.Image,\n        generated_pil_image: Image.Image,\n        prompt: str,\n    ) -&gt; ComputeMetricOutput:\n        \"\"\"Compute the metric for the given images. This is an abstract\n        method and must be overriden by the child class implementation.\n\n        Args:\n            ground_truth_pil_image (Image.Image): Ground truth image in PIL format.\n            generated_pil_image (Image.Image): Generated image in PIL format.\n            prompt (str): Prompt for the image generation.\n\n        Returns:\n            ComputeMetricOutput: Output containing the metric score and ground truth image.\n        \"\"\"\n        pass\n\n    def evaluate(\n        self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n    ) -&gt; Dict[str, float]:\n        \"\"\"Compute the metric for the given images. This method is used as the scorer\n        function for `weave.Evaluation` in the evaluation pipelines.\n\n        Args:\n            prompt (str): Prompt for the image generation.\n            ground_truth_image (str): Ground truth image in base64 format.\n            model_output (Dict[str, Any]): Model output containing the generated image.\n\n        Returns:\n            Union[float, Dict[str, float]]: Metric score.\n        \"\"\"\n        metric_output = self.compute_metric(\n            ground_truth_image, model_output[\"image\"], prompt\n        )\n        self.scores.append(metric_output.score)\n        return {self.name: metric_output.score}\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric.__init__","title":"<code>__init__(name)</code>","text":"<p>Base class for Image Quality Metrics.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Base class for Image Quality Metrics.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n    super().__init__()\n    self.scores = []\n    self.name = name\n    self.config = {}\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric.compute_metric","title":"<code>compute_metric(ground_truth_pil_image, generated_pil_image, prompt)</code>  <code>abstractmethod</code>","text":"<p>Compute the metric for the given images. This is an abstract method and must be overriden by the child class implementation.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_pil_image</code> <code>Image</code> <p>Ground truth image in PIL format.</p> required <code>generated_pil_image</code> <code>Image</code> <p>Generated image in PIL format.</p> required <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <p>Returns:</p> Name Type Description <code>ComputeMetricOutput</code> <code>ComputeMetricOutput</code> <p>Output containing the metric score and ground truth image.</p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>@abstractmethod\ndef compute_metric(\n    self,\n    ground_truth_pil_image: Image.Image,\n    generated_pil_image: Image.Image,\n    prompt: str,\n) -&gt; ComputeMetricOutput:\n    \"\"\"Compute the metric for the given images. This is an abstract\n    method and must be overriden by the child class implementation.\n\n    Args:\n        ground_truth_pil_image (Image.Image): Ground truth image in PIL format.\n        generated_pil_image (Image.Image): Generated image in PIL format.\n        prompt (str): Prompt for the image generation.\n\n    Returns:\n        ComputeMetricOutput: Output containing the metric score and ground truth image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric.evaluate","title":"<code>evaluate(prompt, ground_truth_image, model_output)</code>","text":"<p>Compute the metric for the given images. This method is used as the scorer function for <code>weave.Evaluation</code> in the evaluation pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <code>ground_truth_image</code> <code>str</code> <p>Ground truth image in base64 format.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>Model output containing the generated image.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Union[float, Dict[str, float]]: Metric score.</p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>def evaluate(\n    self, prompt: str, ground_truth_image: Image.Image, model_output: Dict[str, Any]\n) -&gt; Dict[str, float]:\n    \"\"\"Compute the metric for the given images. This method is used as the scorer\n    function for `weave.Evaluation` in the evaluation pipelines.\n\n    Args:\n        prompt (str): Prompt for the image generation.\n        ground_truth_image (str): Ground truth image in base64 format.\n        model_output (Dict[str, Any]): Model output containing the generated image.\n\n    Returns:\n        Union[float, Dict[str, float]]: Metric score.\n    \"\"\"\n    metric_output = self.compute_metric(\n        ground_truth_image, model_output[\"image\"], prompt\n    )\n    self.scores.append(metric_output.score)\n    return {self.name: metric_output.score}\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.ComputeMetricOutput","title":"<code>ComputeMetricOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output of the metric computation function.</p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>class ComputeMetricOutput(BaseModel):\n    \"\"\"Output of the metric computation function.\"\"\"\n\n    score: Union[float, Dict[str, float]]\n    ground_truth_image: str\n</code></pre>"},{"location":"metrics/prompt_image_alignment/","title":"Prompt-image Alignment Metrics","text":""},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.CLIPImageQualityScoreMetric","title":"<code>CLIPImageQualityScoreMetric</code>","text":"<p>               Bases: <code>BasePromptAlignmentMetric</code></p> <p>CLIP Image Quality Assessment metric for to measuring the visual content of images.</p> <p>The metric is based on the CLIP model, which is a neural network trained on a variety of (image, text) pairs to be able to generate a vector representation of the image and the text that is similar if the image and text are semantically similar.</p> <p>The metric works by calculating the cosine similarity between user provided images and pre-defined prompts. The prompts always comes in pairs of \u201cpositive\u201d and \u201cnegative\u201d such as \u201cGood photo.\u201d and \u201cBad photo.\u201d. By calculating the similartity between image embeddings and both the \u201cpositive\u201d and \u201cnegative\u201d prompt, the metric can determine which prompt the image is more similar to. The metric then returns the probability that the image is more similar to the first prompt than the second prompt.</p> <p>Parameters:</p> Name Type Description Default <code>clip_model_name_or_path</code> <code>str</code> <p>The name or path of the CLIP model to use. Defaults to \"clip_iqa\".</p> <code>'clip_iqa'</code> <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"clip_image_quality_assessment\".</p> <code>'clip_image_quality_assessment'</code> Source code in <code>hemm/metrics/prompt_alignment/clip_iqa_score.py</code> <pre><code>class CLIPImageQualityScoreMetric(BasePromptAlignmentMetric):\n    \"\"\"[CLIP Image Quality Assessment](https://arxiv.org/abs/2207.12396) metric\n    for to measuring the visual content of images.\n\n    The metric is based on the [CLIP](https://arxiv.org/abs/2103.00020) model,\n    which is a neural network trained on a variety of (image, text) pairs to be\n    able to generate a vector representation of the image and the text that is\n    similar if the image and text are semantically similar.\n\n    The metric works by calculating the cosine similarity between user provided images\n    and pre-defined prompts. The prompts always comes in pairs of \u201cpositive\u201d and \u201cnegative\u201d\n    such as \u201cGood photo.\u201d and \u201cBad photo.\u201d. By calculating the similartity between image\n    embeddings and both the \u201cpositive\u201d and \u201cnegative\u201d prompt, the metric can determine which\n    prompt the image is more similar to. The metric then returns the probability that the\n    image is more similar to the first prompt than the second prompt.\n\n    Args:\n        clip_model_name_or_path (str, optional): The name or path of the CLIP model to use.\n            Defaults to \"clip_iqa\".\n        name (str, optional): Name of the metric. Defaults to \"clip_image_quality_assessment\".\n    \"\"\"\n\n    def __init__(\n        self,\n        clip_model_name_or_path: str = \"clip_iqa\",\n        name: str = \"clip_image_quality_assessment\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.clip_iqa_fn = partial(\n            clip_image_quality_assessment, model_name_or_path=clip_model_name_or_path\n        )\n        self.built_in_prompts = [\n            \"quality\",\n            \"brightness\",\n            \"noisiness\",\n            \"colorfullness\",\n            \"sharpness\",\n            \"contrast\",\n            \"complexity\",\n            \"natural\",\n            \"happy\",\n            \"scary\",\n            \"new\",\n            \"real\",\n            \"beautiful\",\n            \"lonely\",\n            \"relaxing\",\n        ]\n        self.config = {\"clip_model_name_or_path\": clip_model_name_or_path}\n\n    @weave.op()\n    def compute_metric(\n        self, pil_image: Image, prompt: str\n    ) -&gt; Union[float, Dict[str, float]]:\n        images = np.expand_dims(np.array(pil_image), axis=0).astype(np.uint8) / 255.0\n        score_dict = {}\n        for prompt in tqdm(\n            self.built_in_prompts, desc=\"Calculating IQA scores\", leave=False\n        ):\n            clip_iqa_score = float(\n                self.clip_iqa_fn(\n                    images=torch.from_numpy(images).permute(0, 3, 1, 2),\n                    prompts=tuple([prompt] * images.shape[0]),\n                ).detach()\n            )\n            score_dict[f\"{self.name}_{prompt}\"] = clip_iqa_score\n        return score_dict\n\n    @weave.op()\n    def evaluate(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, float]:\n        _ = \"CLIPImageQualityScoreMetric\"\n        return super().evaluate(prompt, model_output)\n\n    @weave.op()\n    async def evaluate_async(\n        self, prompt: str, model_output: Dict[str, Any]\n    ) -&gt; Dict[str, float]:\n        _ = \"CLIPImageQualityScoreMetric\"\n        return self.evaluate(prompt, model_output)\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.CLIPScoreMetric","title":"<code>CLIPScoreMetric</code>","text":"<p>               Bases: <code>BasePromptAlignmentMetric</code></p> <p>CLIP score metric for text-to-image similarity. CLIP Score is a reference free metric that can be used to evaluate the correlation between a generated caption for an image and the actual content of the image. It has been found to be highly correlated with human judgement.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"clip_score\".</p> <code>'clip_score'</code> <code>clip_model_name_or_path</code> <code>str</code> <p>The name or path of the CLIP model to use. Defaults to \"openai/clip-vit-base-patch16\".</p> <code>'openai/clip-vit-base-patch16'</code> Source code in <code>hemm/metrics/prompt_alignment/clip_score.py</code> <pre><code>class CLIPScoreMetric(BasePromptAlignmentMetric):\n    \"\"\"[CLIP score](https://arxiv.org/abs/2104.08718) metric for text-to-image similarity.\n    CLIP Score is a reference free metric that can be used to evaluate the correlation between\n    a generated caption for an image and the actual content of the image. It has been found to\n    be highly correlated with human judgement.\n\n    Args:\n        name (str, optional): Name of the metric. Defaults to \"clip_score\".\n        clip_model_name_or_path (str, optional): The name or path of the CLIP model to use.\n            Defaults to \"openai/clip-vit-base-patch16\".\n    \"\"\"\n\n    def __init__(\n        self,\n        clip_model_name_or_path: str = \"openai/clip-vit-base-patch16\",\n        name: str = \"clip_score\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.clip_score_fn = partial(\n            clip_score, model_name_or_path=clip_model_name_or_path\n        )\n        self.config = {\"clip_model_name_or_path\": clip_model_name_or_path}\n\n    @weave.op()\n    def compute_metric(\n        self, pil_image: Image.Image, prompt: str\n    ) -&gt; Union[float, Dict[str, float]]:\n        images = np.expand_dims(np.array(pil_image), axis=0)\n        return float(\n            self.clip_score_fn(\n                torch.from_numpy(images).permute(0, 3, 1, 2), prompt\n            ).detach()\n        )\n\n    @weave.op()\n    def evaluate(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, float]:\n        _ = \"CLIPScoreMetric\"\n        return super().evaluate(prompt, model_output)\n\n    @weave.op()\n    async def evaluate_async(\n        self, prompt: str, model_output: Dict[str, Any]\n    ) -&gt; Dict[str, float]:\n        _ = \"CLIPScoreMetric\"\n        return self.evaluate(prompt, model_output)\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.base.BasePromptAlignmentMetric","title":"<code>BasePromptAlignmentMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Base class for Prompt Alignment Metrics.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>hemm/metrics/prompt_alignment/base.py</code> <pre><code>class BasePromptAlignmentMetric(BaseMetric):\n    \"\"\"Base class for Prompt Alignment Metrics.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        super().__init__()\n        self.scores = []\n        self.name = name\n        self.config = {}\n\n    @abstractmethod\n    def compute_metric(\n        self, pil_image: Image.Image, prompt: str\n    ) -&gt; Union[float, Dict[str, float]]:\n        \"\"\"Compute the metric for the given image. This is an abstract\n        method and must be overriden by the child class implementation.\n\n        Args:\n            pil_image (Image.Image): Image in PIL format.\n            prompt (str): Prompt for the image generation.\n\n        Returns:\n            Union[float, Dict[str, float]]: Metric score.\n        \"\"\"\n        pass\n\n    def evaluate(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, float]:\n        \"\"\"Compute the metric for the given image. This method is used as the scorer\n        function for `weave.Evaluation` in the evaluation pipelines.\n\n        Args:\n            prompt (str): Prompt for the image generation.\n            model_output (Dict[str, Any]): Model output containing the generated image.\n\n        Returns:\n            Dict[str, float]: Metric score.\n        \"\"\"\n        score = self.compute_metric(model_output[\"image\"], prompt)\n        self.scores.append(score)\n        return {self.name: score}\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.base.BasePromptAlignmentMetric.compute_metric","title":"<code>compute_metric(pil_image, prompt)</code>  <code>abstractmethod</code>","text":"<p>Compute the metric for the given image. This is an abstract method and must be overriden by the child class implementation.</p> <p>Parameters:</p> Name Type Description Default <code>pil_image</code> <code>Image</code> <p>Image in PIL format.</p> required <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <p>Returns:</p> Type Description <code>Union[float, Dict[str, float]]</code> <p>Union[float, Dict[str, float]]: Metric score.</p> Source code in <code>hemm/metrics/prompt_alignment/base.py</code> <pre><code>@abstractmethod\ndef compute_metric(\n    self, pil_image: Image.Image, prompt: str\n) -&gt; Union[float, Dict[str, float]]:\n    \"\"\"Compute the metric for the given image. This is an abstract\n    method and must be overriden by the child class implementation.\n\n    Args:\n        pil_image (Image.Image): Image in PIL format.\n        prompt (str): Prompt for the image generation.\n\n    Returns:\n        Union[float, Dict[str, float]]: Metric score.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.base.BasePromptAlignmentMetric.evaluate","title":"<code>evaluate(prompt, model_output)</code>","text":"<p>Compute the metric for the given image. This method is used as the scorer function for <code>weave.Evaluation</code> in the evaluation pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>Model output containing the generated image.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Metric score.</p> Source code in <code>hemm/metrics/prompt_alignment/base.py</code> <pre><code>def evaluate(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, float]:\n    \"\"\"Compute the metric for the given image. This method is used as the scorer\n    function for `weave.Evaluation` in the evaluation pipelines.\n\n    Args:\n        prompt (str): Prompt for the image generation.\n        model_output (Dict[str, Any]): Model output containing the generated image.\n\n    Returns:\n        Dict[str, float]: Metric score.\n    \"\"\"\n    score = self.compute_metric(model_output[\"image\"], prompt)\n    self.scores.append(score)\n    return {self.name: score}\n</code></pre>"},{"location":"metrics/spatial_relationship/","title":"Spatial Relationship Metrics","text":"<p>This module aims to implement the Spatial relationship metric described in section 3.2 of T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> Using an object-detection model for spatial relationship evaluation as proposed in T2I-CompBench Weave gives us a holistic view of the evaluations to drill into individual ouputs and scores. <p>Example</p>"},{"location":"metrics/spatial_relationship/#step-1-generate-evaluation-dataset","title":"Step 1: Generate evaluation dataset","text":"<p>Generate an evaluation dataset using the MSCOCO object vocabulary and publish it as a Weave Dataset. You can follow this notebook to learn about the porocess.</p>"},{"location":"metrics/spatial_relationship/#step-2-evaluate","title":"Step 2: Evaluate","text":"<pre><code>import wandb\nimport weave\n\nfrom hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\nfrom hemm.metrics.image_quality import LPIPSMetric, PSNRMetric, SSIMMetric\n\n# Initialize Weave and WandB\nwandb.init(project=\"image-quality-leaderboard\", job_type=\"evaluation\")\nweave.init(project_name=\"image-quality-leaderboard\")\n\n# Initialize the diffusion model to be evaluated as a `weave.Model` using `BaseWeaveModel`\nmodel = BaseDiffusionModel(diffusion_model_name_or_path=\"CompVis/stable-diffusion-v1-4\")\n\n# Add the model to the evaluation pipeline\nevaluation_pipeline = EvaluationPipeline(model=model)\n\n# Define the judge model for 2d spatial relationship metric\njudge = DETRSpatialRelationShipJudge(\n    model_address=detr_model_address, revision=detr_revision\n)\n\n# Add 2d spatial relationship Metric to the evaluation pipeline\nmetric = SpatialRelationshipMetric2D(judge=judge, name=\"2d_spatial_relationship_score\")\nevaluation_pipeline.add_metric(metric)\n\n# Evaluate!\nevaluation_pipeline(dataset=\"t2i_compbench_spatial_prompts:v0\")\n</code></pre>"},{"location":"metrics/spatial_relationship/#metrics","title":"Metrics","text":""},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.spatial_relationship_2d.SpatialRelationshipMetric2D","title":"<code>SpatialRelationshipMetric2D</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Spatial relationship metric for image generation as proposed in Section 4.2 from the paper T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> Sample usage <pre><code>import wandb\nimport weave\n\nfrom hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\nfrom hemm.metrics.image_quality import LPIPSMetric, PSNRMetric, SSIMMetric\n\n# Initialize Weave and WandB\nwandb.init(project=\"image-quality-leaderboard\", job_type=\"evaluation\")\nweave.init(project_name=\"image-quality-leaderboard\")\n\n# Initialize the diffusion model to be evaluated as a `weave.Model` using `BaseWeaveModel`\nmodel = BaseDiffusionModel(diffusion_model_name_or_path=\"CompVis/stable-diffusion-v1-4\")\n\n# Add the model to the evaluation pipeline\nevaluation_pipeline = EvaluationPipeline(model=model)\n\n# Define the judge model for 2d spatial relationship metric\njudge = DETRSpatialRelationShipJudge(\n    model_address=detr_model_address, revision=detr_revision\n)\n\n# Add 2d spatial relationship Metric to the evaluation pipeline\nmetric = SpatialRelationshipMetric2D(judge=judge, name=\"2d_spatial_relationship_score\")\nevaluation_pipeline.add_metric(metric)\n\n# Evaluate!\nevaluation_pipeline(dataset=\"t2i_compbench_spatial_prompts:v0\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>judge</code> <code>Union[Model, DETRSpatialRelationShipJudge]</code> <p>The judge model to predict the bounding boxes from the generated image.</p> required <code>iou_threshold</code> <code>Optional[float]</code> <p>The IoU threshold for the spatial relationship.</p> <code>0.1</code> <code>distance_threshold</code> <code>Optional[float]</code> <p>The distance threshold for the spatial relationship.</p> <code>150</code> <code>name</code> <code>Optional[str]</code> <p>The name of the metric.</p> <code>'spatial_relationship_score'</code> Source code in <code>hemm/metrics/spatial_relationship/spatial_relationship_2d.py</code> <pre><code>class SpatialRelationshipMetric2D(BaseMetric):\n    \"\"\"Spatial relationship metric for image generation as proposed in Section 4.2 from the paper\n    [T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation](https://arxiv.org/pdf/2307.06350).\n\n    ??? example \"Sample usage\"\n        ```python\n        import wandb\n        import weave\n\n        from hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\n        from hemm.metrics.image_quality import LPIPSMetric, PSNRMetric, SSIMMetric\n\n        # Initialize Weave and WandB\n        wandb.init(project=\"image-quality-leaderboard\", job_type=\"evaluation\")\n        weave.init(project_name=\"image-quality-leaderboard\")\n\n        # Initialize the diffusion model to be evaluated as a `weave.Model` using `BaseWeaveModel`\n        model = BaseDiffusionModel(diffusion_model_name_or_path=\"CompVis/stable-diffusion-v1-4\")\n\n        # Add the model to the evaluation pipeline\n        evaluation_pipeline = EvaluationPipeline(model=model)\n\n        # Define the judge model for 2d spatial relationship metric\n        judge = DETRSpatialRelationShipJudge(\n            model_address=detr_model_address, revision=detr_revision\n        )\n\n        # Add 2d spatial relationship Metric to the evaluation pipeline\n        metric = SpatialRelationshipMetric2D(judge=judge, name=\"2d_spatial_relationship_score\")\n        evaluation_pipeline.add_metric(metric)\n\n        # Evaluate!\n        evaluation_pipeline(dataset=\"t2i_compbench_spatial_prompts:v0\")\n        ```\n\n    Args:\n        judge (Union[weave.Model, DETRSpatialRelationShipJudge]): The judge model to predict\n            the bounding boxes from the generated image.\n        iou_threshold (Optional[float], optional): The IoU threshold for the spatial relationship.\n        distance_threshold (Optional[float], optional): The distance threshold for the spatial relationship.\n        name (Optional[str], optional): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        judge: Union[weave.Model, DETRSpatialRelationShipJudge],\n        iou_threshold: Optional[float] = 0.1,\n        distance_threshold: Optional[float] = 150,\n        name: Optional[str] = \"spatial_relationship_score\",\n    ) -&gt; None:\n        super().__init__()\n        self.judge = judge\n        self.judge_config = self.judge.model_dump(mode=\"json\")\n        self.iou_threshold = iou_threshold\n        self.distance_threshold = distance_threshold\n        self.name = name\n        self.scores = []\n        self.config = judge.model_dump()\n\n    @weave.op()\n    def compose_judgement(\n        self,\n        prompt: str,\n        image: Image.Image,\n        entity_1: str,\n        entity_2: str,\n        relationship: str,\n        boxes: List[BoundingBox],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compose the judgement based on the response and the predicted bounding boxes.\n\n        Args:\n            prompt (str): The prompt using which the image was generated.\n            image (Image.Image): The input image.\n            entity_1 (str): First entity.\n            entity_2 (str): Second entity.\n            relationship (str): Relationship between the entities.\n            boxes (List[BoundingBox]): The predicted bounding boxes.\n\n        Returns:\n            Dict[str, Any]: The comprehensive spatial relationship judgement.\n        \"\"\"\n        _ = prompt\n\n        # Determine presence of entities in the judgement\n        judgement = {\n            \"entity_1_present\": False,\n            \"entity_2_present\": False,\n        }\n        entity_1_box: BoundingBox = None\n        entity_2_box: BoundingBox = None\n        annotated_image = image\n        for box in boxes:\n            if box.label == entity_1:\n                judgement[\"entity_1_present\"] = True\n                entity_1_box = box\n            elif box.label == entity_2:\n                judgement[\"entity_2_present\"] = True\n                entity_2_box = box\n            annotated_image = annotate_with_bounding_box(annotated_image, box)\n\n        judgement[\"score\"] = 0.0\n        # assign score based on the spatial relationship inferred from the judgement\n        if judgement[\"entity_1_present\"] and judgement[\"entity_2_present\"]:\n            center_distance_x = abs(\n                entity_1_box.box_coordinates_center.x\n                - entity_2_box.box_coordinates_center.x\n            )\n            center_distance_y = abs(\n                entity_1_box.box_coordinates_center.y\n                - entity_2_box.box_coordinates_center.y\n            )\n            iou = get_iou(entity_1_box, entity_2_box)\n            score = 0.0\n            if relationship in [\"near\", \"next to\", \"on side of\", \"side of\"]:\n                if (\n                    abs(center_distance_x) &lt; self.distance_threshold\n                    or abs(center_distance_y) &lt; self.distance_threshold\n                ):\n                    score = 1.0\n                else:\n                    score = self.distance_threshold / max(\n                        abs(center_distance_x), abs(center_distance_y)\n                    )\n            elif relationship == \"on the right of\":\n                if center_distance_x &lt; 0:\n                    if (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1.0\n                    elif (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n            elif relationship == \"on the left of\":\n                if center_distance_x &gt; 0:\n                    if (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1.0\n                    elif (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n                else:\n                    score = 0.0\n            elif relationship == \"on the bottom of\":\n                if center_distance_y &lt; 0:\n                    if (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1\n                    elif (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n            elif relationship == \"on the top of\":\n                if center_distance_y &gt; 0:\n                    if (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1\n                    elif (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n            judgement[\"score\"] = score\n\n        self.scores.append(\n            {\n                **judgement,\n                **{\n                    \"judge_annotated_image\": wandb.Image(annotated_image),\n                    \"judge_config\": self.judge_config,\n                },\n            }\n        )\n        return {\n            **judgement,\n            **{\"judge_annotated_image\": annotated_image},\n            \"judge_config\": self.judge_config,\n        }\n\n    @weave.op()\n    def evaluate(\n        self,\n        prompt: str,\n        entity_1: str,\n        entity_2: str,\n        relationship: str,\n        model_output: Dict[str, Any],\n    ) -&gt; Dict[str, Union[bool, float, int]]:\n        \"\"\"Calculate the spatial relationship score for the given prompt and model output.\n\n        Args:\n            prompt (str): The prompt for the model.\n            entity_1 (str): The first entity in the spatial relationship.\n            entity_2 (str): The second entity in the spatial relationship.\n            relationship (str): The spatial relationship between the two entities.\n            model_output (Dict[str, Any]): The output from the model.\n\n        Returns:\n            Dict[str, Union[bool, float, int]]: The comprehensive spatial relationship judgement.\n        \"\"\"\n        _ = prompt\n\n        image = model_output[\"image\"]\n        boxes: List[BoundingBox] = self.judge.predict(image)\n        judgement = self.compose_judgement(\n            prompt, image, entity_1, entity_2, relationship, boxes\n        )\n        return {self.name: judgement[\"score\"]}\n\n    @weave.op()\n    async def evaluate_async(\n        self,\n        prompt: str,\n        entity_1: str,\n        entity_2: str,\n        relationship: str,\n        model_output: Dict[str, Any],\n    ) -&gt; Dict[str, Union[bool, float, int]]:\n        return self.evaluate(prompt, entity_1, entity_2, relationship, model_output)\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.spatial_relationship_2d.SpatialRelationshipMetric2D.compose_judgement","title":"<code>compose_judgement(prompt, image, entity_1, entity_2, relationship, boxes)</code>","text":"<p>Compose the judgement based on the response and the predicted bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt using which the image was generated.</p> required <code>image</code> <code>Image</code> <p>The input image.</p> required <code>entity_1</code> <code>str</code> <p>First entity.</p> required <code>entity_2</code> <code>str</code> <p>Second entity.</p> required <code>relationship</code> <code>str</code> <p>Relationship between the entities.</p> required <code>boxes</code> <code>List[BoundingBox]</code> <p>The predicted bounding boxes.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The comprehensive spatial relationship judgement.</p> Source code in <code>hemm/metrics/spatial_relationship/spatial_relationship_2d.py</code> <pre><code>@weave.op()\ndef compose_judgement(\n    self,\n    prompt: str,\n    image: Image.Image,\n    entity_1: str,\n    entity_2: str,\n    relationship: str,\n    boxes: List[BoundingBox],\n) -&gt; Dict[str, Any]:\n    \"\"\"Compose the judgement based on the response and the predicted bounding boxes.\n\n    Args:\n        prompt (str): The prompt using which the image was generated.\n        image (Image.Image): The input image.\n        entity_1 (str): First entity.\n        entity_2 (str): Second entity.\n        relationship (str): Relationship between the entities.\n        boxes (List[BoundingBox]): The predicted bounding boxes.\n\n    Returns:\n        Dict[str, Any]: The comprehensive spatial relationship judgement.\n    \"\"\"\n    _ = prompt\n\n    # Determine presence of entities in the judgement\n    judgement = {\n        \"entity_1_present\": False,\n        \"entity_2_present\": False,\n    }\n    entity_1_box: BoundingBox = None\n    entity_2_box: BoundingBox = None\n    annotated_image = image\n    for box in boxes:\n        if box.label == entity_1:\n            judgement[\"entity_1_present\"] = True\n            entity_1_box = box\n        elif box.label == entity_2:\n            judgement[\"entity_2_present\"] = True\n            entity_2_box = box\n        annotated_image = annotate_with_bounding_box(annotated_image, box)\n\n    judgement[\"score\"] = 0.0\n    # assign score based on the spatial relationship inferred from the judgement\n    if judgement[\"entity_1_present\"] and judgement[\"entity_2_present\"]:\n        center_distance_x = abs(\n            entity_1_box.box_coordinates_center.x\n            - entity_2_box.box_coordinates_center.x\n        )\n        center_distance_y = abs(\n            entity_1_box.box_coordinates_center.y\n            - entity_2_box.box_coordinates_center.y\n        )\n        iou = get_iou(entity_1_box, entity_2_box)\n        score = 0.0\n        if relationship in [\"near\", \"next to\", \"on side of\", \"side of\"]:\n            if (\n                abs(center_distance_x) &lt; self.distance_threshold\n                or abs(center_distance_y) &lt; self.distance_threshold\n            ):\n                score = 1.0\n            else:\n                score = self.distance_threshold / max(\n                    abs(center_distance_x), abs(center_distance_y)\n                )\n        elif relationship == \"on the right of\":\n            if center_distance_x &lt; 0:\n                if (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1.0\n                elif (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n        elif relationship == \"on the left of\":\n            if center_distance_x &gt; 0:\n                if (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1.0\n                elif (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n            else:\n                score = 0.0\n        elif relationship == \"on the bottom of\":\n            if center_distance_y &lt; 0:\n                if (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1\n                elif (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n        elif relationship == \"on the top of\":\n            if center_distance_y &gt; 0:\n                if (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1\n                elif (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n        judgement[\"score\"] = score\n\n    self.scores.append(\n        {\n            **judgement,\n            **{\n                \"judge_annotated_image\": wandb.Image(annotated_image),\n                \"judge_config\": self.judge_config,\n            },\n        }\n    )\n    return {\n        **judgement,\n        **{\"judge_annotated_image\": annotated_image},\n        \"judge_config\": self.judge_config,\n    }\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.spatial_relationship_2d.SpatialRelationshipMetric2D.evaluate","title":"<code>evaluate(prompt, entity_1, entity_2, relationship, model_output)</code>","text":"<p>Calculate the spatial relationship score for the given prompt and model output.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt for the model.</p> required <code>entity_1</code> <code>str</code> <p>The first entity in the spatial relationship.</p> required <code>entity_2</code> <code>str</code> <p>The second entity in the spatial relationship.</p> required <code>relationship</code> <code>str</code> <p>The spatial relationship between the two entities.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>The output from the model.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[bool, float, int]]</code> <p>Dict[str, Union[bool, float, int]]: The comprehensive spatial relationship judgement.</p> Source code in <code>hemm/metrics/spatial_relationship/spatial_relationship_2d.py</code> <pre><code>@weave.op()\ndef evaluate(\n    self,\n    prompt: str,\n    entity_1: str,\n    entity_2: str,\n    relationship: str,\n    model_output: Dict[str, Any],\n) -&gt; Dict[str, Union[bool, float, int]]:\n    \"\"\"Calculate the spatial relationship score for the given prompt and model output.\n\n    Args:\n        prompt (str): The prompt for the model.\n        entity_1 (str): The first entity in the spatial relationship.\n        entity_2 (str): The second entity in the spatial relationship.\n        relationship (str): The spatial relationship between the two entities.\n        model_output (Dict[str, Any]): The output from the model.\n\n    Returns:\n        Dict[str, Union[bool, float, int]]: The comprehensive spatial relationship judgement.\n    \"\"\"\n    _ = prompt\n\n    image = model_output[\"image\"]\n    boxes: List[BoundingBox] = self.judge.predict(image)\n    judgement = self.compose_judgement(\n        prompt, image, entity_1, entity_2, relationship, boxes\n    )\n    return {self.name: judgement[\"score\"]}\n</code></pre>"},{"location":"metrics/spatial_relationship/#judges","title":"Judges","text":""},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.judges.DETRSpatialRelationShipJudge","title":"<code>DETRSpatialRelationShipJudge</code>","text":"<p>               Bases: <code>Model</code></p> <p>DETR spatial relationship judge model for 2D images.</p> <p>Parameters:</p> Name Type Description Default <code>model_address</code> <code>str</code> <p>The address of the model to use.</p> <code>'facebook/detr-resnet-50'</code> <code>revision</code> <code>str</code> <p>The revision of the model to use.</p> <code>'no_timm'</code> <code>name</code> <code>str</code> <p>The name of the judge model</p> <code>'detr_spatial_relationship_judge'</code> Source code in <code>hemm/metrics/spatial_relationship/judges/detr.py</code> <pre><code>class DETRSpatialRelationShipJudge(weave.Model):\n    \"\"\"[DETR](https://huggingface.co/docs/transformers/en/model_doc/detr) spatial relationship judge model for 2D images.\n\n    Args:\n        model_address (str, optional): The address of the model to use.\n        revision (str, optional): The revision of the model to use.\n        name (str, optional): The name of the judge model\n    \"\"\"\n\n    model_address: str\n    revision: str\n    name: str\n    _feature_extractor: DetrImageProcessor = None\n    _object_detection_model: DetrForObjectDetection = None\n\n    def __init__(\n        self,\n        model_address: str = \"facebook/detr-resnet-50\",\n        revision: str = \"no_timm\",\n        name: str = \"detr_spatial_relationship_judge\",\n    ):\n        super().__init__(model_address=model_address, revision=revision, name=name)\n        self._feature_extractor = DetrImageProcessor.from_pretrained(\n            self.model_address, revision=self.revision\n        )\n        self._object_detection_model = DetrForObjectDetection.from_pretrained(\n            self.model_address, revision=self.revision\n        )\n\n    @weave.op()\n    def predict(self, image: Image.Image) -&gt; List[BoundingBox]:\n        \"\"\"Predict the bounding boxes from the input image.\n\n        Args:\n            image (Image.Image): The input image.\n\n        Returns:\n            List[BoundingBox]: The predicted bounding boxes.\n        \"\"\"\n        encoding = self._feature_extractor(image, return_tensors=\"pt\")\n        outputs = self._object_detection_model(**encoding)\n        target_sizes = torch.tensor([image.size[::-1]])\n        results = self._feature_extractor.post_process_object_detection(\n            outputs, target_sizes=target_sizes, threshold=0.9\n        )[0]\n        bboxes = []\n        for score, label, box in zip(\n            results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n        ):\n            xmin, ymin, xmax, ymax = box.tolist()\n            bboxes.append(\n                BoundingBox(\n                    box_coordinates_min=CartesianCoordinate2D(x=xmin, y=ymin),\n                    box_coordinates_max=CartesianCoordinate2D(x=xmax, y=ymax),\n                    box_coordinates_center=CartesianCoordinate2D(\n                        x=(xmin + xmax) / 2, y=(ymin + ymax) / 2\n                    ),\n                    label=self._object_detection_model.config.id2label[label.item()],\n                    score=score.item(),\n                )\n            )\n        return bboxes\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.judges.DETRSpatialRelationShipJudge.predict","title":"<code>predict(image)</code>","text":"<p>Predict the bounding boxes from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image.</p> required <p>Returns:</p> Type Description <code>List[BoundingBox]</code> <p>List[BoundingBox]: The predicted bounding boxes.</p> Source code in <code>hemm/metrics/spatial_relationship/judges/detr.py</code> <pre><code>@weave.op()\ndef predict(self, image: Image.Image) -&gt; List[BoundingBox]:\n    \"\"\"Predict the bounding boxes from the input image.\n\n    Args:\n        image (Image.Image): The input image.\n\n    Returns:\n        List[BoundingBox]: The predicted bounding boxes.\n    \"\"\"\n    encoding = self._feature_extractor(image, return_tensors=\"pt\")\n    outputs = self._object_detection_model(**encoding)\n    target_sizes = torch.tensor([image.size[::-1]])\n    results = self._feature_extractor.post_process_object_detection(\n        outputs, target_sizes=target_sizes, threshold=0.9\n    )[0]\n    bboxes = []\n    for score, label, box in zip(\n        results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n    ):\n        xmin, ymin, xmax, ymax = box.tolist()\n        bboxes.append(\n            BoundingBox(\n                box_coordinates_min=CartesianCoordinate2D(x=xmin, y=ymin),\n                box_coordinates_max=CartesianCoordinate2D(x=xmax, y=ymax),\n                box_coordinates_center=CartesianCoordinate2D(\n                    x=(xmin + xmax) / 2, y=(ymin + ymax) / 2\n                ),\n                label=self._object_detection_model.config.id2label[label.item()],\n                score=score.item(),\n            )\n        )\n    return bboxes\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.judges.RTDETRSpatialRelationShipJudge","title":"<code>RTDETRSpatialRelationShipJudge</code>","text":"<p>               Bases: <code>Model</code></p> <p>RT-DETR spatial relationship judge model for 2D images.</p> <p>Parameters:</p> Name Type Description Default <code>model_address</code> <code>str</code> <p>The address of the model to use.</p> <code>'facebook/detr-resnet-50'</code> <code>revision</code> <code>str</code> <p>The revision of the model to use.</p> required <code>name</code> <code>str</code> <p>The name of the judge model</p> <code>'detr_spatial_relationship_judge'</code> Source code in <code>hemm/metrics/spatial_relationship/judges/rt_detr.py</code> <pre><code>class RTDETRSpatialRelationShipJudge(weave.Model):\n    \"\"\"[RT-DETR](https://huggingface.co/docs/transformers/en/model_doc/rt_detr) spatial relationship judge model for 2D images.\n\n    Args:\n        model_address (str, optional): The address of the model to use.\n        revision (str, optional): The revision of the model to use.\n        name (str, optional): The name of the judge model\n    \"\"\"\n\n    model_address: str\n    name: str\n    _feature_extractor: RTDetrImageProcessor = None\n    _object_detection_model: RTDetrForObjectDetection = None\n\n    def __init__(\n        self,\n        model_address: str = \"facebook/detr-resnet-50\",\n        name: str = \"detr_spatial_relationship_judge\",\n    ):\n        super().__init__(model_address=model_address, name=name)\n        self._feature_extractor = RTDetrImageProcessor.from_pretrained(\n            self.model_address\n        )\n        self._object_detection_model = RTDetrForObjectDetection.from_pretrained(\n            self.model_address\n        )\n\n    @weave.op()\n    def predict(self, image: Image.Image) -&gt; List[BoundingBox]:\n        \"\"\"Predict the bounding boxes from the input image.\n\n        Args:\n            image (Image.Image): The input image.\n\n        Returns:\n            List[BoundingBox]: The predicted bounding boxes.\n        \"\"\"\n        encoding = self._feature_extractor(image, return_tensors=\"pt\")\n        outputs = self._object_detection_model(**encoding)\n        target_sizes = torch.tensor([image.size[::-1]])\n        results = self._feature_extractor.post_process_object_detection(\n            outputs, target_sizes=target_sizes, threshold=0.9\n        )[0]\n        bboxes = []\n        for score, label, box in zip(\n            results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n        ):\n            xmin, ymin, xmax, ymax = box.tolist()\n            bboxes.append(\n                BoundingBox(\n                    box_coordinates_min=CartesianCoordinate2D(x=xmin, y=ymin),\n                    box_coordinates_max=CartesianCoordinate2D(x=xmax, y=ymax),\n                    box_coordinates_center=CartesianCoordinate2D(\n                        x=(xmin + xmax) / 2, y=(ymin + ymax) / 2\n                    ),\n                    label=self._object_detection_model.config.id2label[label.item()],\n                    score=score.item(),\n                )\n            )\n        return bboxes\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.judges.RTDETRSpatialRelationShipJudge.predict","title":"<code>predict(image)</code>","text":"<p>Predict the bounding boxes from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image.</p> required <p>Returns:</p> Type Description <code>List[BoundingBox]</code> <p>List[BoundingBox]: The predicted bounding boxes.</p> Source code in <code>hemm/metrics/spatial_relationship/judges/rt_detr.py</code> <pre><code>@weave.op()\ndef predict(self, image: Image.Image) -&gt; List[BoundingBox]:\n    \"\"\"Predict the bounding boxes from the input image.\n\n    Args:\n        image (Image.Image): The input image.\n\n    Returns:\n        List[BoundingBox]: The predicted bounding boxes.\n    \"\"\"\n    encoding = self._feature_extractor(image, return_tensors=\"pt\")\n    outputs = self._object_detection_model(**encoding)\n    target_sizes = torch.tensor([image.size[::-1]])\n    results = self._feature_extractor.post_process_object_detection(\n        outputs, target_sizes=target_sizes, threshold=0.9\n    )[0]\n    bboxes = []\n    for score, label, box in zip(\n        results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n    ):\n        xmin, ymin, xmax, ymax = box.tolist()\n        bboxes.append(\n            BoundingBox(\n                box_coordinates_min=CartesianCoordinate2D(x=xmin, y=ymin),\n                box_coordinates_max=CartesianCoordinate2D(x=xmax, y=ymax),\n                box_coordinates_center=CartesianCoordinate2D(\n                    x=(xmin + xmax) / 2, y=(ymin + ymax) / 2\n                ),\n                label=self._object_detection_model.config.id2label[label.item()],\n                score=score.item(),\n            )\n        )\n    return bboxes\n</code></pre>"},{"location":"metrics/notebooks/generate_spatial_relationship_dataset/","title":"Generate spatial relationship dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport jsonlines\n\nimport wandb\nimport weave\n</pre> import os  import jsonlines  import wandb import weave In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"2d-spatial-relationship\")\nweave.init(project_name=\"2d-spatial-relationship\")\n</pre> wandb.init(project=\"2d-spatial-relationship\") weave.init(project_name=\"2d-spatial-relationship\") In\u00a0[\u00a0]: Copied! <pre>mscoco_classes = [\n    \"person\",\n    \"bicycle\",\n    \"car\",\n    \"motorcycle\",\n    \"airplane\",\n    \"bus\",\n    \"train\",\n    \"truck\",\n    \"boat\",\n    \"traffic light\",\n    \"fire hydrant\",\n    \"stop sign\",\n    \"parking meter\",\n    \"bench\",\n    \"bird\",\n    \"cat\",\n    \"dog\",\n    \"horse\",\n    \"sheep\",\n    \"cow\",\n    \"elephant\",\n    \"bear\",\n    \"zebra\",\n    \"giraffe\",\n    \"backpack\",\n    \"umbrella\",\n    \"handbag\",\n    \"tie\",\n    \"suitcase\",\n    \"frisbee\",\n    \"skis\",\n    \"snowboard\",\n    \"sports ball\",\n    \"kite\",\n    \"baseball bat\",\n    \"baseball glove\",\n    \"skateboard\",\n    \"surfboard\",\n    \"tennis racket\",\n    \"bottle\",\n    \"wine glass\",\n    \"cup\",\n    \"fork\",\n    \"knife\",\n    \"spoon\",\n    \"bowl\",\n    \"banana\",\n    \"apple\",\n    \"sandwich\",\n    \"orange\",\n    \"broccoli\",\n    \"carrot\",\n    \"hot dog\",\n    \"pizza\",\n    \"donut\",\n    \"cake\",\n    \"chair\",\n    \"couch\",\n    \"potted plant\",\n    \"bed\",\n    \"dining table\",\n    \"toilet\",\n    \"tv\",\n    \"laptop\",\n    \"mouse\",\n    \"remote\",\n    \"keyboard\",\n    \"cell phone\",\n    \"microwave\",\n    \"oven\",\n    \"toaster\",\n    \"sink\",\n    \"refrigerator\",\n    \"book\",\n    \"clock\",\n    \"vase\",\n    \"scissors\",\n    \"teddy bear\",\n    \"hair drier\",\n    \"toothbrush\",\n]\n</pre> mscoco_classes = [     \"person\",     \"bicycle\",     \"car\",     \"motorcycle\",     \"airplane\",     \"bus\",     \"train\",     \"truck\",     \"boat\",     \"traffic light\",     \"fire hydrant\",     \"stop sign\",     \"parking meter\",     \"bench\",     \"bird\",     \"cat\",     \"dog\",     \"horse\",     \"sheep\",     \"cow\",     \"elephant\",     \"bear\",     \"zebra\",     \"giraffe\",     \"backpack\",     \"umbrella\",     \"handbag\",     \"tie\",     \"suitcase\",     \"frisbee\",     \"skis\",     \"snowboard\",     \"sports ball\",     \"kite\",     \"baseball bat\",     \"baseball glove\",     \"skateboard\",     \"surfboard\",     \"tennis racket\",     \"bottle\",     \"wine glass\",     \"cup\",     \"fork\",     \"knife\",     \"spoon\",     \"bowl\",     \"banana\",     \"apple\",     \"sandwich\",     \"orange\",     \"broccoli\",     \"carrot\",     \"hot dog\",     \"pizza\",     \"donut\",     \"cake\",     \"chair\",     \"couch\",     \"potted plant\",     \"bed\",     \"dining table\",     \"toilet\",     \"tv\",     \"laptop\",     \"mouse\",     \"remote\",     \"keyboard\",     \"cell phone\",     \"microwave\",     \"oven\",     \"toaster\",     \"sink\",     \"refrigerator\",     \"book\",     \"clock\",     \"vase\",     \"scissors\",     \"teddy bear\",     \"hair drier\",     \"toothbrush\", ] In\u00a0[\u00a0]: Copied! <pre>def compose_prompt(entity_1: str, entity_2: str, relationship: str):\n    numeracy_entity_1 = \"an\" if entity_1[0] in \"aeiou\" else \"a\"\n    numeracy_entity_2 = \"an\" if entity_2[0] in \"aeiou\" else \"a\"\n    return f\"{numeracy_entity_1} {entity_1} {relationship} {numeracy_entity_2} {entity_2}\"\n\n\nspatial_relationship_rows = []\nrelationships = [\n    \"near\",\n    \"next to\",\n    \"on side of\",\n    \"side of\",\n    \"on the right of\",\n    \"on the left of\",\n    \"on the bottom of\",\n    \"on the top of\"\n]\ntable = wandb.Table(columns=[\"prompt\", \"entity_1\", \"entity_2\", \"relationship\"])\nfor entity_1 in mscoco_classes:\n    for entity_2 in mscoco_classes:\n        if entity_1 == entity_2:\n            continue\n        for relationship in relationships:\n            row = {\n                \"prompt\": compose_prompt(entity_1, entity_2, relationship),\n                \"entity_1\": entity_1,\n                \"entity_2\": entity_2,\n                \"relationship\": relationship,\n            }\n            spatial_relationship_rows.append(row)\n            table.add_data(row[\"prompt\"], row[\"entity_1\"], row[\"entity_2\"], row[\"relationship\"])\n</pre> def compose_prompt(entity_1: str, entity_2: str, relationship: str):     numeracy_entity_1 = \"an\" if entity_1[0] in \"aeiou\" else \"a\"     numeracy_entity_2 = \"an\" if entity_2[0] in \"aeiou\" else \"a\"     return f\"{numeracy_entity_1} {entity_1} {relationship} {numeracy_entity_2} {entity_2}\"   spatial_relationship_rows = [] relationships = [     \"near\",     \"next to\",     \"on side of\",     \"side of\",     \"on the right of\",     \"on the left of\",     \"on the bottom of\",     \"on the top of\" ] table = wandb.Table(columns=[\"prompt\", \"entity_1\", \"entity_2\", \"relationship\"]) for entity_1 in mscoco_classes:     for entity_2 in mscoco_classes:         if entity_1 == entity_2:             continue         for relationship in relationships:             row = {                 \"prompt\": compose_prompt(entity_1, entity_2, relationship),                 \"entity_1\": entity_1,                 \"entity_2\": entity_2,                 \"relationship\": relationship,             }             spatial_relationship_rows.append(row)             table.add_data(row[\"prompt\"], row[\"entity_1\"], row[\"entity_2\"], row[\"relationship\"]) In\u00a0[\u00a0]: Copied! <pre>with jsonlines.open(os.path.join(\"dataset.jsonl\"), mode=\"w\") as writer:\n    writer.write(spatial_relationship_rows)\n\ndataset_description = \"\"\"A dataset of prompts for evaluation 2D spatial relationships between objects in images.\nThe dataset is generated using the vocabulary of objects from the [MSCOCO](https://cocodataset.org) dataset.\nThe idea for this dataset is inspired by the\n[T2I-Compbench 2D Spatial Relationships dataset](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/huangky_connect_hku_hk/Er_BhrcMwGREht6gnKGIErMBx8H8yRXLDfWgWQwKaObQ4w?e=YzT5wG).\n\"\"\"\n\nartifact = wandb.Artifact(\n    name=\"2d-spatial-prompts-mscoco\",\n    type=\"dataset\",\n    metadata={\n        \"format\": \"jsonl\",\n        \"description\": dataset_description,\n    }\n)\nartifact.add_file(local_path=os.path.join(\"dataset.jsonl\"))\nwandb.log_artifact(artifact)\n\nwandb.log({\"dataset/2d_spatial_prompts\": table})\n</pre> with jsonlines.open(os.path.join(\"dataset.jsonl\"), mode=\"w\") as writer:     writer.write(spatial_relationship_rows)  dataset_description = \"\"\"A dataset of prompts for evaluation 2D spatial relationships between objects in images. The dataset is generated using the vocabulary of objects from the [MSCOCO](https://cocodataset.org) dataset. The idea for this dataset is inspired by the [T2I-Compbench 2D Spatial Relationships dataset](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/huangky_connect_hku_hk/Er_BhrcMwGREht6gnKGIErMBx8H8yRXLDfWgWQwKaObQ4w?e=YzT5wG). \"\"\"  artifact = wandb.Artifact(     name=\"2d-spatial-prompts-mscoco\",     type=\"dataset\",     metadata={         \"format\": \"jsonl\",         \"description\": dataset_description,     } ) artifact.add_file(local_path=os.path.join(\"dataset.jsonl\")) wandb.log_artifact(artifact)  wandb.log({\"dataset/2d_spatial_prompts\": table}) In\u00a0[\u00a0]: Copied! <pre>spatial_relationship_dataset = weave.Dataset(\n    name=\"2d-spatial-prompts-mscoco\",\n    rows=spatial_relationship_rows,\n    description=dataset_description,\n)\nweave.publish(spatial_relationship_dataset)\n</pre> spatial_relationship_dataset = weave.Dataset(     name=\"2d-spatial-prompts-mscoco\",     rows=spatial_relationship_rows,     description=dataset_description, ) weave.publish(spatial_relationship_dataset) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\nos.remove(\"dataset.jsonl\")\n</pre> wandb.finish() os.remove(\"dataset.jsonl\")"},{"location":"metrics/vqa/disentangled_vqa/","title":"Disentangled VQA","text":"<p>This module aims to implement the Disentangled VQA metric inspired by Section 4.1 from the paper T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> Using the disentangled BLIP-VQA model for attribute-binding evaluation as proposed in T2I-CompBench Weave gives us a holistic view of the evaluations to drill into individual ouputs and scores. <p>Example</p>"},{"location":"metrics/vqa/disentangled_vqa/#step-1-generate-evaluation-dataset","title":"Step 1: Generate evaluation dataset","text":"<p>Generate the dataset consisting of prompts in the format <code>\u201ca {adj_1} {noun_1} and a {adj_2} {noun_2}\u201d</code> and the corresponding metadata using an LLM capable of generating json objects like GPT4-O. The dataset is then published both as a W&amp;B dataset artifact and as a weave dataset.</p> <pre><code>from hemm.metrics.attribute_binding import AttributeBindingDatasetGenerator\n\n    dataset_generator = AttributeBindingDatasetGenerator(\n        openai_model=\"gpt-4o\",\n        openai_seed=42,\n        num_prompts_in_single_call=20,\n        num_api_calls=50,\n        project_name=\"disentangled_vqa\",\n    )\n\n    dataset_generator(dump_dir=\"./dump\")\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#step-2-evaluate","title":"Step 2: Evaluate","text":"<pre><code>import wandb\nimport weave\n\nwandb.init(project=project, entity=entity, job_type=\"evaluation\")\nweave.init(project_name=project)\n\ndiffusion_model = BaseDiffusionModel(\n    diffusion_model_name_or_path=diffusion_model_address,\n    enable_cpu_offfload=diffusion_model_enable_cpu_offfload,\n    image_height=image_size[0],\n    image_width=image_size[1],\n)\nevaluation_pipeline = EvaluationPipeline(model=diffusion_model)\n\njudge = BlipVQAJudge()\nmetric = DisentangledVQAMetric(judge=judge, name=\"disentangled_blip_metric\")\nevaluation_pipeline.add_metric(metric)\n\nevaluation_pipeline(dataset=dataset)\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#metrics","title":"Metrics","text":""},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.disentangled_vqa.DisentangledVQAMetric","title":"<code>DisentangledVQAMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Disentangled VQA metric to evaluate the attribute-binding capability for image generation models as proposed in Section 4.1 from the paper T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> Sample usage <pre><code>import wandb\nimport weave\nfrom hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\nfrom hemm.metrics.vqa import DisentangledVQAMetric\nfrom hemm.metrics.vqa.judges import BlipVQAJudge\n\nwandb.init(project=project, entity=entity, job_type=\"evaluation\")\nweave.init(project_name=project)\n\ndiffusion_model = BaseDiffusionModel(\n    diffusion_model_name_or_path=diffusion_model_address,\n    enable_cpu_offfload=diffusion_model_enable_cpu_offfload,\n    image_height=image_size[0],\n    image_width=image_size[1],\n)\nevaluation_pipeline = EvaluationPipeline(model=diffusion_model)\n\njudge = BlipVQAJudge()\nmetric = DisentangledVQAMetric(judge=judge, name=\"disentangled_blip_metric\")\nevaluation_pipeline.add_metric(metric)\n\nevaluation_pipeline(dataset=dataset)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>judge</code> <code>Union[Model, BlipVQAJudge]</code> <p>The judge model to evaluate the attribute-binding capability.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the metric. Defaults to \"disentangled_vlm_metric\".</p> <code>'disentangled_vlm_metric'</code> Source code in <code>hemm/metrics/vqa/disentangled_vqa.py</code> <pre><code>class DisentangledVQAMetric(BaseMetric):\n    \"\"\"Disentangled VQA metric to evaluate the attribute-binding capability\n    for image generation models as proposed in Section 4.1 from the paper\n    [T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation](https://arxiv.org/pdf/2307.06350).\n\n    ??? example \"Sample usage\"\n        ```python\n        import wandb\n        import weave\n        from hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\n        from hemm.metrics.vqa import DisentangledVQAMetric\n        from hemm.metrics.vqa.judges import BlipVQAJudge\n\n        wandb.init(project=project, entity=entity, job_type=\"evaluation\")\n        weave.init(project_name=project)\n\n        diffusion_model = BaseDiffusionModel(\n            diffusion_model_name_or_path=diffusion_model_address,\n            enable_cpu_offfload=diffusion_model_enable_cpu_offfload,\n            image_height=image_size[0],\n            image_width=image_size[1],\n        )\n        evaluation_pipeline = EvaluationPipeline(model=diffusion_model)\n\n        judge = BlipVQAJudge()\n        metric = DisentangledVQAMetric(judge=judge, name=\"disentangled_blip_metric\")\n        evaluation_pipeline.add_metric(metric)\n\n        evaluation_pipeline(dataset=dataset)\n        ```\n\n    Args:\n        judge (Union[weave.Model, BlipVQAJudge]): The judge model to evaluate the attribute-binding capability.\n        name (Optional[str]): The name of the metric. Defaults to \"disentangled_vlm_metric\".\n    \"\"\"\n\n    def __init__(\n        self,\n        judge: Union[weave.Model, BlipVQAJudge],\n        name: Optional[str] = \"disentangled_vlm_metric\",\n    ) -&gt; None:\n        super().__init__()\n        self.judge = judge\n        self.config = self.judge.model_dump()\n        self.scores = []\n        self.name = name\n\n    @weave.op()\n    def evaluate(\n        self,\n        prompt: str,\n        adj_1: str,\n        noun_1: str,\n        adj_2: str,\n        noun_2: str,\n        model_output: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate the attribute-binding capability of the model.\n\n        Args:\n            prompt (str): The prompt for the model.\n            adj_1 (str): The first adjective.\n            noun_1 (str): The first noun.\n            adj_2 (str): The second adjective.\n            noun_2 (str): The second noun.\n            model_output (Dict[str, Any]): The model output.\n\n        Returns:\n            Dict[str, Any]: The evaluation result.\n        \"\"\"\n        _ = prompt\n        judgement = self.judge.predict(\n            adj_1, noun_1, adj_2, noun_2, model_output[\"image\"]\n        )\n        self.scores.append(judgement)\n        return judgement\n\n    @weave.op()\n    async def evaluate_async(\n        self,\n        prompt: str,\n        adj_1: str,\n        noun_1: str,\n        adj_2: str,\n        noun_2: str,\n        model_output: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        return self.evaluate(prompt, adj_1, noun_1, adj_2, noun_2, model_output)\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.disentangled_vqa.DisentangledVQAMetric.evaluate","title":"<code>evaluate(prompt, adj_1, noun_1, adj_2, noun_2, model_output)</code>","text":"<p>Evaluate the attribute-binding capability of the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt for the model.</p> required <code>adj_1</code> <code>str</code> <p>The first adjective.</p> required <code>noun_1</code> <code>str</code> <p>The first noun.</p> required <code>adj_2</code> <code>str</code> <p>The second adjective.</p> required <code>noun_2</code> <code>str</code> <p>The second noun.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>The model output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The evaluation result.</p> Source code in <code>hemm/metrics/vqa/disentangled_vqa.py</code> <pre><code>@weave.op()\ndef evaluate(\n    self,\n    prompt: str,\n    adj_1: str,\n    noun_1: str,\n    adj_2: str,\n    noun_2: str,\n    model_output: Dict[str, Any],\n) -&gt; Dict[str, Any]:\n    \"\"\"Evaluate the attribute-binding capability of the model.\n\n    Args:\n        prompt (str): The prompt for the model.\n        adj_1 (str): The first adjective.\n        noun_1 (str): The first noun.\n        adj_2 (str): The second adjective.\n        noun_2 (str): The second noun.\n        model_output (Dict[str, Any]): The model output.\n\n    Returns:\n        Dict[str, Any]: The evaluation result.\n    \"\"\"\n    _ = prompt\n    judgement = self.judge.predict(\n        adj_1, noun_1, adj_2, noun_2, model_output[\"image\"]\n    )\n    self.scores.append(judgement)\n    return judgement\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#judges","title":"Judges","text":""},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.judges.blip_vqa.BlipVQAJudge","title":"<code>BlipVQAJudge</code>","text":"<p>               Bases: <code>Model</code></p> <p>Weave Model to judge the presence of entities in an image using the Blip-VQA model.</p> <p>Parameters:</p> Name Type Description Default <code>blip_processor_address</code> <code>str</code> <p>The address of the BlipProcessor model.</p> <code>'Salesforce/blip-vqa-base'</code> <code>blip_vqa_address</code> <code>str</code> <p>The address of the BlipForQuestionAnswering model.</p> <code>'Salesforce/blip-vqa-base'</code> <code>device</code> <code>str</code> <p>The device to use for inference</p> <code>'cuda'</code> Source code in <code>hemm/metrics/vqa/judges/blip_vqa.py</code> <pre><code>class BlipVQAJudge(weave.Model):\n    \"\"\"Weave Model to judge the presence of entities in an image using the\n    [Blip-VQA model](https://huggingface.co/Salesforce/blip-vqa-base).\n\n    Args:\n        blip_processor_address (str): The address of the BlipProcessor model.\n        blip_vqa_address (str): The address of the BlipForQuestionAnswering model.\n        device (str): The device to use for inference\n    \"\"\"\n\n    blip_processor_address: str\n    blip_vqa_address: str\n    device: str\n    _torch_dtype: torch.dtype = torch.float32\n    _blip_processor_model: BlipProcessor = None\n    _blip_vqa_model: BlipForQuestionAnswering = None\n\n    def __init__(\n        self,\n        blip_processor_address: str = \"Salesforce/blip-vqa-base\",\n        blip_vqa_address: str = \"Salesforce/blip-vqa-base\",\n        device: str = \"cuda\",\n    ):\n        super().__init__(\n            blip_processor_address=blip_processor_address,\n            blip_vqa_address=blip_vqa_address,\n            device=device,\n        )\n        self._blip_processor_model = BlipProcessor.from_pretrained(\n            self.blip_processor_address\n        )\n        self._blip_vqa_model = BlipForQuestionAnswering.from_pretrained(\n            self.blip_vqa_address, torch_dtype=self._torch_dtype\n        ).to(self.device)\n\n    def _get_probability(self, target_token: str, scores: List[torch.Tensor]) -&gt; float:\n        target_token_id = self._blip_processor_model.tokenizer.convert_tokens_to_ids(\n            target_token\n        )\n        probabilities = [F.softmax(score, dim=-1) for score in scores]\n        target_token_probabilities = [\n            prob[:, target_token_id].item() for prob in probabilities\n        ]\n        max_target_token_probability = max(target_token_probabilities)\n        return max_target_token_probability\n\n    @weave.op()\n    def get_target_token_probability(\n        self, question: str, image: Image.Image\n    ) -&gt; Dict[str, float]:\n        inputs = self._blip_processor_model(image, question, return_tensors=\"pt\").to(\n            self.device\n        )\n        with torch.no_grad():\n            generated_ids = self._blip_vqa_model.generate(\n                **inputs, output_scores=True, return_dict_in_generate=True\n            )\n        scores = generated_ids.scores\n        yes_probability = self._get_probability(\"yes\", scores)\n        no_probability = self._get_probability(\"no\", scores)\n        return {\n            \"yes_proba\": yes_probability,\n            \"no_proba\": no_probability,\n            \"present\": yes_probability &gt; no_probability,\n        }\n\n    @weave.op()\n    def predict(\n        self, adj_1: str, noun_1: str, adj_2: str, noun_2: str, image: Image.Image\n    ) -&gt; Dict:\n        \"\"\"Predict the probabilities presence of entities in an image using the Blip-VQA model.\n\n        Args:\n            adj_1 (str): The adjective of the first entity.\n            noun_1 (str): The noun of the first entity.\n            adj_2 (str): The adjective of the second entity.\n            noun_2 (str): The noun of the second entity.\n            image (Image.Image): The input image.\n\n        Returns:\n            Dict: The probabilities of the presence of the entities.\n        \"\"\"\n        question_1 = f\"is {adj_1} {noun_1} present in the picture?\"\n        question_2 = f\"is {adj_2} {noun_2} present in the picture?\"\n        return {\n            \"entity_1\": self.get_target_token_probability(question_1, image),\n            \"entity_2\": self.get_target_token_probability(question_2, image),\n        }\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.judges.blip_vqa.BlipVQAJudge.predict","title":"<code>predict(adj_1, noun_1, adj_2, noun_2, image)</code>","text":"<p>Predict the probabilities presence of entities in an image using the Blip-VQA model.</p> <p>Parameters:</p> Name Type Description Default <code>adj_1</code> <code>str</code> <p>The adjective of the first entity.</p> required <code>noun_1</code> <code>str</code> <p>The noun of the first entity.</p> required <code>adj_2</code> <code>str</code> <p>The adjective of the second entity.</p> required <code>noun_2</code> <code>str</code> <p>The noun of the second entity.</p> required <code>image</code> <code>Image</code> <p>The input image.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>The probabilities of the presence of the entities.</p> Source code in <code>hemm/metrics/vqa/judges/blip_vqa.py</code> <pre><code>@weave.op()\ndef predict(\n    self, adj_1: str, noun_1: str, adj_2: str, noun_2: str, image: Image.Image\n) -&gt; Dict:\n    \"\"\"Predict the probabilities presence of entities in an image using the Blip-VQA model.\n\n    Args:\n        adj_1 (str): The adjective of the first entity.\n        noun_1 (str): The noun of the first entity.\n        adj_2 (str): The adjective of the second entity.\n        noun_2 (str): The noun of the second entity.\n        image (Image.Image): The input image.\n\n    Returns:\n        Dict: The probabilities of the presence of the entities.\n    \"\"\"\n    question_1 = f\"is {adj_1} {noun_1} present in the picture?\"\n    question_2 = f\"is {adj_2} {noun_2} present in the picture?\"\n    return {\n        \"entity_1\": self.get_target_token_probability(question_1, image),\n        \"entity_2\": self.get_target_token_probability(question_2, image),\n    }\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#dataset-generation","title":"Dataset Generation","text":""},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.dataset_generator.attribute_binding.AttributeBindingDatasetGenerator","title":"<code>AttributeBindingDatasetGenerator</code>","text":"<p>Dataset generator for evaluation of attribute binding capability of image-generation models. This class enables us to generate the dataset consisting of prompts in the format <code>\u201ca {adj_1} {noun_1} and a {adj_2} {noun_2}\u201d</code> and the corresponding metadata using an LLM capable of generating json objects like GPT4-O. The dataset is then published both as a W&amp;B dataset artifact and as a weave dataset.</p> Sample usage <pre><code>from hemm.metrics.vqa import AttributeBindingDatasetGenerator\n\ndataset_generator = AttributeBindingDatasetGenerator(\n    openai_model=\"gpt-4o\",\n    openai_seed=42,\n    num_prompts_in_single_call=20,\n    num_api_calls=50,\n    project_name=\"disentangled_vqa\",\n)\n\ndataset_generator(dump_dir=\"./dump\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>openai_model</code> <code>Optional[str]</code> <p>The OpenAI model to use for generating prompts.</p> <code>'gpt-3.5-turbo'</code> <code>openai_seed</code> <code>Optional[Union[int, List[int]]]</code> <p>Seed to use for generating prompts. If not provided, seeds will be auto-generated.</p> <code>None</code> <code>num_prompts_in_single_call</code> <code>Optional[int]</code> <p>Number of prompts to generate in a single API call.</p> <code>20</code> <code>num_api_calls</code> <code>Optional[int]</code> <p>Number of API calls to make.</p> <code>50</code> <code>project_name</code> <code>Optional[str]</code> <p>Name of the Weave project to use for logging the dataset.</p> <code>'diffusion_leaderboard'</code> Source code in <code>hemm/metrics/vqa/dataset_generator/attribute_binding.py</code> <pre><code>class AttributeBindingDatasetGenerator:\n    \"\"\"Dataset generator for evaluation of attribute binding capability of image-generation models.\n    This class enables us to generate the dataset consisting of prompts in the format\n    `\u201ca {adj_1} {noun_1} and a {adj_2} {noun_2}\u201d` and the corresponding metadata using an LLM capable\n    of generating json objects like GPT4-O. The dataset is then published both as a\n    [W&amp;B dataset artifact](https://docs.wandb.ai/guides/artifacts) and as a\n    [weave dataset](https://wandb.github.io/weave/guides/core-types/datasets).\n\n    ??? example \"Sample usage\"\n        ```python\n        from hemm.metrics.vqa import AttributeBindingDatasetGenerator\n\n        dataset_generator = AttributeBindingDatasetGenerator(\n            openai_model=\"gpt-4o\",\n            openai_seed=42,\n            num_prompts_in_single_call=20,\n            num_api_calls=50,\n            project_name=\"disentangled_vqa\",\n        )\n\n        dataset_generator(dump_dir=\"./dump\")\n        ```\n\n    Args:\n        openai_model (Optional[str]): The OpenAI model to use for generating prompts.\n        openai_seed (Optional[Union[int, List[int]]]): Seed to use for generating prompts.\n            If not provided, seeds will be auto-generated.\n        num_prompts_in_single_call (Optional[int]): Number of prompts to generate in a single API call.\n        num_api_calls (Optional[int]): Number of API calls to make.\n        project_name (Optional[str]): Name of the Weave project to use for logging the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        openai_model: Optional[str] = \"gpt-3.5-turbo\",\n        openai_seed: Optional[Union[int, List[int]]] = None,\n        num_prompts_in_single_call: Optional[int] = 20,\n        num_api_calls: Optional[int] = 50,\n        project_name: Optional[str] = \"diffusion_leaderboard\",\n    ) -&gt; None:\n        if not openai_seed:\n            self.openai_seeds = [autogenerate_seed() for _ in range(num_api_calls)]\n        elif isinstance(openai_seed, int):\n            self.openai_seeds = [openai_seed] * num_api_calls\n        elif isinstance(openai_seed, list) and len(openai_seed) != num_api_calls:\n            raise ValueError(\n                \"Length of `openai_seed` should be equal to `num_api_calls`\"\n            )\n        self.attribute_binding_model = AttributeBindingModel(\n            openai_model=openai_model,\n            num_prompts=num_prompts_in_single_call,\n        )\n        self.model_configs = self.attribute_binding_model.model_dump()\n        self.attribute_binding_model._initialize()\n        self.num_prompts_in_single_call = num_prompts_in_single_call\n        self.num_api_calls = num_api_calls\n        self.project_name = project_name\n        self.dataset_rows: List[Dict] = []\n        self.wandb_table = wandb.Table(\n            columns=[\"prompt\", \"adj_1\", \"noun_1\", \"adj_2\", \"noun_2\"]\n        )\n\n    def publish_dataset(self, dump_dir: str):\n        os.makedirs(dump_dir, exist_ok=True)\n        weave_dataset = weave.Dataset(\n            name=\"attribute_binding_dataset\", rows=self.dataset_rows\n        )\n        weave.publish(weave_dataset)\n        with jsonlines.open(\n            os.path.join(dump_dir, \"dataset.jsonl\"), mode=\"w\"\n        ) as writer:\n            writer.write(self.dataset_rows)\n        artifact = wandb.Artifact(name=\"attribute_binding_dataset\", type=\"dataset\")\n        artifact.add_file(local_path=os.path.join(dump_dir, \"dataset.jsonl\"))\n        wandb.log_artifact(artifact)\n        wandb.log({\"dataset/attribute_binding\": self.wandb_table})\n\n    @weave.op()\n    async def evaluate_generated_response(\n        self, prompt: str, model_output: Dict\n    ) -&gt; Dict:\n        eval_response = AttributeBindingEvaluationResponse()\n        model_output = str_to_json(model_output[\"response\"])\n        if model_output:\n            if \"data\" not in model_output:\n                return eval_response.model_dump()\n            model_output = model_output[\"data\"]\n            eval_response.is_correct_json = True\n            for idx in model_output:\n                prompt = idx[\"sentence\"]\n                adj_1 = idx[\"metadata\"][\"adj_1\"]\n                adj_2 = idx[\"metadata\"][\"adj_2\"]\n                noun_1 = idx[\"metadata\"][\"noun_1\"]\n                noun_2 = idx[\"metadata\"][\"noun_2\"]\n                self.wandb_table.add_data(prompt, adj_1, noun_1, adj_2, noun_2)\n                self.dataset_rows.append(\n                    {\n                        \"prompt\": prompt,\n                        \"adj_1\": adj_1,\n                        \"noun_1\": noun_1,\n                        \"adj_2\": adj_2,\n                        \"noun_2\": noun_2,\n                    }\n                )\n                if f\"a {adj_1} {noun_1} and a {adj_2} {noun_2}\" == prompt:\n                    eval_response.num_correct_predictions += 1\n            eval_response.total_predictions = len(model_output)\n            eval_response.accuracy = eval_response.num_correct_predictions / len(\n                model_output\n            )\n        return eval_response.model_dump()\n\n    def __call__(self, dump_dir: Optional[str] = \"./dump\") -&gt; None:\n        \"\"\"Generate the dataset and publish it to Weave.\n\n        Args:\n            dump_dir (Optional[str]): Directory to dump the dataset.\n        \"\"\"\n        wandb.init(\n            project=self.project_name,\n            job_type=\"attribute_binding_dataset\",\n            config=self.model_configs,\n        )\n        weave.init(project_name=self.project_name)\n        evaluation = weave.Evaluation(\n            dataset=[{\"prompt\": \"\", \"seed\": seed} for seed in self.openai_seeds],\n            scorers=[self.evaluate_generated_response],\n        )\n        with weave.attributes(self.model_configs):\n            asyncio.run(evaluation.evaluate(self.attribute_binding_model.predict))\n        self.publish_dataset(dump_dir)\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.dataset_generator.attribute_binding.AttributeBindingDatasetGenerator.__call__","title":"<code>__call__(dump_dir='./dump')</code>","text":"<p>Generate the dataset and publish it to Weave.</p> <p>Parameters:</p> Name Type Description Default <code>dump_dir</code> <code>Optional[str]</code> <p>Directory to dump the dataset.</p> <code>'./dump'</code> Source code in <code>hemm/metrics/vqa/dataset_generator/attribute_binding.py</code> <pre><code>def __call__(self, dump_dir: Optional[str] = \"./dump\") -&gt; None:\n    \"\"\"Generate the dataset and publish it to Weave.\n\n    Args:\n        dump_dir (Optional[str]): Directory to dump the dataset.\n    \"\"\"\n    wandb.init(\n        project=self.project_name,\n        job_type=\"attribute_binding_dataset\",\n        config=self.model_configs,\n    )\n    weave.init(project_name=self.project_name)\n    evaluation = weave.Evaluation(\n        dataset=[{\"prompt\": \"\", \"seed\": seed} for seed in self.openai_seeds],\n        scorers=[self.evaluate_generated_response],\n    )\n    with weave.attributes(self.model_configs):\n        asyncio.run(evaluation.evaluate(self.attribute_binding_model.predict))\n    self.publish_dataset(dump_dir)\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.dataset_generator.attribute_binding.AttributeBindingModel","title":"<code>AttributeBindingModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>Weave Model to generate prompts for evaluation of attribute binding capability of image-generation models using an OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>openai_model</code> <code>Optional[str]</code> <p>The OpenAI model to use for generating prompts.</p> required <code>num_prompts</code> <code>Optional[int]</code> <p>Number of prompts to generate.</p> required Source code in <code>hemm/metrics/vqa/dataset_generator/attribute_binding.py</code> <pre><code>class AttributeBindingModel(weave.Model):\n    \"\"\"Weave Model to generate prompts for evaluation of attribute binding capability of\n    image-generation models using an OpenAI model.\n\n    Args:\n        openai_model (Optional[str]): The OpenAI model to use for generating prompts.\n        num_prompts (Optional[int]): Number of prompts to generate.\n    \"\"\"\n\n    openai_model: Optional[str] = \"gpt-3.5-turbo\"\n    num_prompts: Optional[int] = 20\n    _openai_client: Optional[OpenAI] = None\n    _system_prompt: Optional[str] = None\n    _user_prompt: Optional[str] = None\n\n    def _initialize(self):\n        if self._system_prompt is None:\n            self._system_prompt = \"\"\"\n            You are a helpful assistant designed to generate some sentences and additional metadata in JSON format.\n            \"\"\"\n        if self._user_prompt is None:\n            self._user_prompt = (\n                \"\"\"\n            Please generate prompts in the format of \u201ca {adj_1} {noun_1} and a {adj_2} {noun_2}\u201d\n            by using the shape adj.: long, tall, short, big, small, cubic, cylindrical,\n            pyramidal, round, circular, oval, oblong, spherical, triangular, square, rectangular,\n            conical, pentagonal, teardrop, crescent, and diamond.\n\n            The output should be a list of \"\"\"\n                + str(self.num_prompts)\n                + \"\"\" JSONs like the following:\n\n            \\{\n                \"data\": \\[\n                    \\{\n                        \"sentence\": \"a long balloon and a short giraffe\",\n                        \"metadata\": \\{\n                            \"adj_1\": \"long\",\n                            \"noun_1\": \"balloon\",\n                            \"adj_2\": \"short\",\n                            \"noun_2\": \"giraffe\"\n                        \\}\n                    \\},\n                    \\{\n                        \"sentence\": \"a tall suitcase and a small frog\",\n                        \"metadata\": \\{\n                            \"adj_1\": \"tall\",\n                            \"noun_1\": \"suitcase\",\n                            \"adj_2\": \"small\",\n                            \"noun_2\": \"frog\"\n                        \\}\n                    \\},\n                    \\{\n                        \"sentence\": \"a big horse and a small man\",\n                        \"metadata\": \\{\n                            \"adj_1\": \"big\",\n                            \"noun_1\": \"horse\",\n                            \"adj_2\": \"small\",\n                            \"noun_2\": \"man\",\n                        \\}\n                    \\}\n                \\],\n            \\}\n            \"\"\"\n            )\n        self._openai_client = OpenAI()\n\n    @weave.op()\n    def predict(self, seed: int) -&gt; Dict[str, str]:\n        \"\"\"Generate prompts and corresponding metadata for evaluation of attribute binding\n        capability of image-generation models.\n\n        Args:\n            seed (int): OpenAI seed to use for generating prompts.\n        \"\"\"\n        return {\n            \"response\": self._openai_client.chat.completions.create(\n                model=self.openai_model,\n                response_format={\"type\": \"json_object\"},\n                seed=seed,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": self._system_prompt,\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": self._user_prompt,\n                    },\n                ],\n            )\n            .choices[0]\n            .message.content\n        }\n</code></pre>"},{"location":"metrics/vqa/disentangled_vqa/#hemm.metrics.vqa.dataset_generator.attribute_binding.AttributeBindingModel.predict","title":"<code>predict(seed)</code>","text":"<p>Generate prompts and corresponding metadata for evaluation of attribute binding capability of image-generation models.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>OpenAI seed to use for generating prompts.</p> required Source code in <code>hemm/metrics/vqa/dataset_generator/attribute_binding.py</code> <pre><code>@weave.op()\ndef predict(self, seed: int) -&gt; Dict[str, str]:\n    \"\"\"Generate prompts and corresponding metadata for evaluation of attribute binding\n    capability of image-generation models.\n\n    Args:\n        seed (int): OpenAI seed to use for generating prompts.\n    \"\"\"\n    return {\n        \"response\": self._openai_client.chat.completions.create(\n            model=self.openai_model,\n            response_format={\"type\": \"json_object\"},\n            seed=seed,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": self._system_prompt,\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": self._user_prompt,\n                },\n            ],\n        )\n        .choices[0]\n        .message.content\n    }\n</code></pre>"},{"location":"metrics/vqa/multi_modal_llm/","title":"Multi-modal LLM Based Evaluation","text":"<p>This module aims to implement the Multi-modal LLM based metric inspired by</p> <ul> <li>Section IV.D of the paper T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation and</li> <li>Section 4.4 of the paper T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</li> </ul> Using Multi-modal LLM based metric for evaluation a diffusion model. The Weave UI gives us a holistic view of the evaluations to drill into individual ouputs and scores. <p>Example</p> <p>First, download the Spacy English langugage pipeline <pre><code>python -m spacy download en_core_web_sm\n</code></pre> Next, you need to set your OpenAI API key: <pre><code>export OPENAI_API_KEY=\"&lt;INSERT-YOUR-OPENAI-API-KEY&gt;\"\n</code></pre> Finallly, you can run the following snippet to evaluate your model: <pre><code>import wandb\nimport weave\n\nfrom hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\nfrom hemm.metrics.vqa import MultiModalLLMEvaluationMetric\nfrom hemm.metrics.vqa.judges.mmllm_judges import OpenAIJudge, PromptCategory\n\nwandb.init(project=\"mllm-eval\", job_type=\"evaluation\")\nweave.init(project_name=\"mllm-eval\")\n\ndataset = weave.ref(dataset_ref).get()\n\ndiffusion_model = BaseDiffusionModel(\n    diffusion_model_name_or_path=\"stabilityai/stable-diffusion-2-1\",\n    enable_cpu_offfload=False,\n    image_height=512,\n    image_width=512,\n)\nevaluation_pipeline = EvaluationPipeline(model=diffusion_model)\n\njudge = OpenAIJudge(prompt_property=PromptCategory.complex)\nmetric = MultiModalLLMEvaluationMetric(judge=judge)\nevaluation_pipeline.add_metric(metric)\n\nevaluation_pipeline(dataset=dataset)\n</code></pre></p>"},{"location":"metrics/vqa/multi_modal_llm/#metrics","title":"Metrics","text":""},{"location":"metrics/vqa/multi_modal_llm/#hemm.metrics.vqa.multi_modal_llm_eval.MultiModalLLMEvaluationMetric","title":"<code>MultiModalLLMEvaluationMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Multi-modal LLM-based evaluation metric for an image-generation model.</p> <p>Parameters:</p> Name Type Description Default <code>judge</code> <code>Union[Model, OpenAIJudge]</code> <p>The judge LLM model to evaluate the generated images.</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the evaluation.</p> <code>'mmllm_eval_metric'</code> Source code in <code>hemm/metrics/vqa/multi_modal_llm_eval.py</code> <pre><code>class MultiModalLLMEvaluationMetric(BaseMetric):\n    \"\"\"Multi-modal LLM-based evaluation metric for an image-generation model.\n\n    Args:\n        judge (Union[weave.Model, OpenAIJudge]): The judge LLM model to evaluate the generated images.\n        name (Optional[str]): Name of the evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        judge: Union[weave.Model, OpenAIJudge],\n        name: Optional[str] = \"mmllm_eval_metric\",\n    ) -&gt; None:\n        super().__init__()\n        self.judge = judge\n        self.config = self.judge.model_dump()\n        self.prompt_property = judge.prompt_property\n        self.scores = []\n        self.name = name\n\n    @weave.op()\n    def evaluate(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate the generated image using the judge LLM model.\n\n        Args:\n            prompt (str): The prompt for the model.\n            model_output (Dict[str, Any]): The model output.\n        \"\"\"\n        judgements: List[OpenAIJudgeMent] = self.judge.predict(\n            prompt=prompt, image=model_output[\"image\"]\n        )\n        score = sum([judgement.judgement.score for judgement in judgements])\n        fractional_score = sum([judgement.fractional_score for judgement in judgements])\n        evaluation_dict = {\n            \"score\": score / len(judgements),\n            \"fractional_score\": fractional_score / len(judgements),\n        }\n        self.scores.append(evaluation_dict)\n        return evaluation_dict\n\n    @weave.op()\n    async def evaluate_async(\n        self, prompt: str, model_output: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        return self.evaluate(prompt, model_output)\n</code></pre>"},{"location":"metrics/vqa/multi_modal_llm/#hemm.metrics.vqa.multi_modal_llm_eval.MultiModalLLMEvaluationMetric.evaluate","title":"<code>evaluate(prompt, model_output)</code>","text":"<p>Evaluate the generated image using the judge LLM model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt for the model.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>The model output.</p> required Source code in <code>hemm/metrics/vqa/multi_modal_llm_eval.py</code> <pre><code>@weave.op()\ndef evaluate(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Evaluate the generated image using the judge LLM model.\n\n    Args:\n        prompt (str): The prompt for the model.\n        model_output (Dict[str, Any]): The model output.\n    \"\"\"\n    judgements: List[OpenAIJudgeMent] = self.judge.predict(\n        prompt=prompt, image=model_output[\"image\"]\n    )\n    score = sum([judgement.judgement.score for judgement in judgements])\n    fractional_score = sum([judgement.fractional_score for judgement in judgements])\n    evaluation_dict = {\n        \"score\": score / len(judgements),\n        \"fractional_score\": fractional_score / len(judgements),\n    }\n    self.scores.append(evaluation_dict)\n    return evaluation_dict\n</code></pre>"},{"location":"metrics/vqa/multi_modal_llm/#judges","title":"Judges","text":""},{"location":"metrics/vqa/multi_modal_llm/#hemm.metrics.vqa.judges.mmllm_judges.openai_judge.OpenAIJudge","title":"<code>OpenAIJudge</code>","text":"<p>               Bases: <code>Model</code></p> <p>OpenAI judge model for evaluating the generated images. The model uses OpenAI's GPT-4 model to evaluate the alignment of the generated images to the respective prompts using a chain-of-thought prompting strategy. The model is inspired by Section IV.D of the paper T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation and Section 4.4 of the paper T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_pipeline</code> <code>str</code> <p>The Spacy pipeline to use for extracting the prompt parts.</p> <code>'en_core_web_sm'</code> <code>prompt_property</code> <code>PromptCategory</code> <p>The property of the prompt to evaluate.</p> <code>color</code> <code>openai_model</code> <code>str</code> <p>The OpenAI model to use for evaluation.</p> <code>'gpt-4o-2024-08-06'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for the OpenAI model.</p> <code>5</code> <code>seed</code> <code>int</code> <p>Seed value for the random number generator.</p> <code>42</code> <code>system_prompt</code> <code>Optional[str]</code> <p>The system prompt for the OpenAI model</p> required Source code in <code>hemm/metrics/vqa/judges/mmllm_judges/openai_judge.py</code> <pre><code>class OpenAIJudge(weave.Model):\n    \"\"\"OpenAI judge model for evaluating the generated images. The model uses\n    OpenAI's GPT-4 model to evaluate the alignment of the generated images to\n    the respective prompts using a chain-of-thought prompting strategy. The model\n    is inspired by Section IV.D of the paper\n    [T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation](https://karine-h.github.io/T2I-CompBench-new/)\n    and Section 4.4 of the paper\n    [T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation](https://arxiv.org/abs/2307.06350).\n\n    Args:\n        prompt_pipeline (str): The Spacy pipeline to use for extracting the prompt parts.\n        prompt_property (PromptCategory): The property of the prompt to evaluate.\n        openai_model (str): The OpenAI model to use for evaluation.\n        max_retries (int): The maximum number of retries for the OpenAI model.\n        seed (int): Seed value for the random number generator.\n        system_prompt (Optional[str]): The system prompt for the OpenAI model\n    \"\"\"\n\n    prompt_pipeline: str\n    prompt_property: PromptCategory\n    openai_model: str\n    max_retries: int\n    seed: int\n    _nlp_pipeline: spacy.Language = None\n    _openai_client: OpenAI = None\n    _total_score: int = 4\n\n    def __init__(\n        self,\n        prompt_pipeline: str = \"en_core_web_sm\",\n        prompt_property: PromptCategory = PromptCategory.color,\n        openai_model: str = \"gpt-4o-2024-08-06\",\n        max_retries: int = 5,\n        seed: int = 42,\n    ):\n        super().__init__(\n            prompt_pipeline=prompt_pipeline,\n            prompt_property=prompt_property,\n            openai_model=openai_model,\n            max_retries=max_retries,\n            seed=seed,\n        )\n        subprocess.run([\"spacy\", \"download\", \"en_core_web_sm\"])\n        self._nlp_pipeline = spacy.load(self.prompt_pipeline)\n        self._openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\n    @weave.op()\n    def extract_prompt_parts(self, prompt: str) -&gt; List[TaggedPromptParts]:\n        \"\"\"Extract the prompt parts from the given prompt.\n\n        Args:\n            prompt (str): The prompt to extract the parts from.\n\n        Returns:\n            List[TaggedPromptParts]: List of tagged prompt objects.\n        \"\"\"\n        doc = self._nlp_pipeline(prompt)\n        tagged_prompt_parts: List[TaggedPromptParts] = []\n        for chunk in doc.noun_chunks:\n            chunk_np = chunk.text\n            for token in list(chunk)[::-1]:\n                if token.pos_ == \"NOUN\":\n                    noun = token.lemma_\n                    adjective = chunk_np.replace(f\" {noun}\", \"\")\n                    adjective = adjective.replace(\"the \", \"\")\n                    adjective = adjective.replace(\"a \", \"\")\n                    adjective = adjective.replace(\"an \", \"\")\n                    break\n            tagged_prompt_parts.append(\n                TaggedPromptParts(entity=chunk_np, noun=noun, adjective=adjective)\n            )\n        return tagged_prompt_parts\n\n    @weave.op()\n    def frame_question(self, prompt: str, image: Image.Image) -&gt; List[JudgeQuestion]:\n        \"\"\"Frame the question corresponding to the given prompt and image for\n        the chain-of-thought system of judgement.\n\n        Args:\n            prompt (str): The prompt to frame the question for.\n            image (Image.Image): The image to frame the question for.\n\n        Returns:\n            List[JudgeQuestion]: List of questions to ask for the given prompt.\n        \"\"\"\n        if self.prompt_property in [PromptCategory.spatial, PromptCategory.spatial_3d]:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the objects and their spatial layout in the image.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to identify objects and their spatial layout in the image.\nYou have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate if the text \\\"{prompt}\\\" is correctly portrayed in the image.\nGive a score from 1 to 5, according to the following criteria:\n\n5: correct spatial layout in the image for all objects mentioned in the text.\n4: basically, spatial layout of objects matches the text.\n3: spatial layout not aligned properly with the text.\n2: image not aligned properly with the text.\n1: image almost irrelevant to the text.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        elif self.prompt_property == PromptCategory.action:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the the actions, events, objects and their relationships in the image.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to identify the actions, events, objects and their relationships in the image.\nYou have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate if the text \\\"{prompt}\\\" is correctly portrayed in the image.\nGive a score from 1 to 5, according to the following criteria:\n\n5: the image accurately portrayed the actions, events and relationships between objects described in the text.\n4: the image portrayed most of the actions, events and relationships but with minor discrepancies.\n3: the image depicted some elements, but action relationships between objects are not correct.\n2: the image failed to convey the full scope of the text.\n1: the image did not depict any actions or events that match the text.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        elif self.prompt_property == PromptCategory.numeracy:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the objects and their quantities in the image.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to identify objects and their quantities in the image.\nYou have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate how well the image aligns with the text prompt: \\\"{prompt}\\\"\nGive a score from 1 to 5, according to the following criteria:\n\n5: correct numerical content in the image for all objects mentioned in the text\n4: basically, numerical content of objects matches the text\n3: numerical content not aligned properly with the text\n2: image not aligned properly with the text\n1: image almost irrelevant to the text\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        elif self.prompt_property == PromptCategory.complex:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the objects in the image and their attributes\n(such as color, shape, texture), spatial layout and action relationships.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to evaluate the correspondence of the image to a given text prompt.\nFocus on the objects in the image and their attributes (such as color, shape, texture),\nspatial layout and action relationships. You have to extract the question, the score, and the\nexplanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate how well the image aligns with the text prompt: \\\"{prompt}\\\"\nGive a score from 1 to 5, according to the following criteria:\n\n5: the image perfectly matches the content of the text prompt with no discrepancies.\n4: the image portrayed most of the actions, events and relationships but with minor discrepancies.\n3: the image depicted some elements in the text prompt, but ignored some key parts or details.\n2: the image did not depict any actions or events that match the text.\n1: the image failed to convey the full scope in the text prompt.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        tagged_prompt_parts = self.extract_prompt_parts(prompt)\n        questions: List[str] = []\n        for tagged_prompt_part in tagged_prompt_parts:\n            question = JudgeQuestion(\n                image_desciption_system_prompt=f\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to any objects and their {self.prompt_property.name} in the given image.\n                \"\"\",\n                judgement_question_system_prompt=f\"\"\"\nYou are a helpful assistant meant to identify any objects and their {self.prompt_property.name}\nin the given image. You have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate if there is a {tagged_prompt_part.entity} in the image.\nGive a score from 1 to 4, according to the following criteria:\n\n4: there is {tagged_prompt_part.noun}, and {self.prompt_property.name} is {tagged_prompt_part.adjective}.\n3: there is {tagged_prompt_part.noun}, {self.prompt_property.name} is mostly {tagged_prompt_part.adjective}.\n2: there is {tagged_prompt_part.noun}, but it is not {tagged_prompt_part.adjective}.\n1: no {tagged_prompt_part.noun} in the image.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            questions.append((question, image))\n        return questions\n\n    @weave.op\n    def execute_chain_of_thought(\n        self, question: JudgeQuestion, image: Image.Image\n    ) -&gt; OpenAIJudgeMent:\n        image_description_explanation = (\n            self._openai_client.chat.completions.create(\n                model=self.openai_model,\n                seed=self.seed,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": question.image_desciption_system_prompt,\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\"url\": base64_encode_image(image)},\n                            },\n                        ],\n                    },\n                ],\n            )\n            .choices[0]\n            .message.content\n        )\n        question.judgement_question += f\"\"\"\n\nHere is a detailed explanation of the image:\n---\n{image_description_explanation}\n---\n\nProvide your analysis and explanation to justify the score.\n        \"\"\"\n        judgement_response = (\n            self._openai_client.beta.chat.completions.parse(\n                model=self.openai_model,\n                response_format=JudgeMent,\n                seed=self.seed,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": question.judgement_question_system_prompt,\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": question.judgement_question},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\"url\": base64_encode_image(image)},\n                            },\n                        ],\n                    },\n                ],\n            )\n            .choices[0]\n            .message.parsed\n        )\n        return judgement_response\n\n    @weave.op()\n    def predict(self, prompt: str, image: Image.Image) -&gt; List[OpenAIJudgeMent]:\n        \"\"\"Predict the score for the given prompt and image.\n\n        Args:\n            prompt (str): The prompt to evaluate.\n            image (Image.Image): The image to evaluate.\n        \"\"\"\n        questions = self.frame_question(prompt, image)\n        answers = []\n        for question, image in questions:\n            judgement_response: JudgeMent = self.execute_chain_of_thought(\n                question, image\n            )\n            judgement_response.explanation = (\n                f\"The score is {judgement_response.score}/{self._total_score}. \"\n                + judgement_response.explanation\n            )\n            answers.append(\n                OpenAIJudgeMent(\n                    judgement=judgement_response,\n                    fractional_score=judgement_response.score / self._total_score,\n                )\n            )\n        return answers\n</code></pre>"},{"location":"metrics/vqa/multi_modal_llm/#hemm.metrics.vqa.judges.mmllm_judges.openai_judge.OpenAIJudge.extract_prompt_parts","title":"<code>extract_prompt_parts(prompt)</code>","text":"<p>Extract the prompt parts from the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to extract the parts from.</p> required <p>Returns:</p> Type Description <code>List[TaggedPromptParts]</code> <p>List[TaggedPromptParts]: List of tagged prompt objects.</p> Source code in <code>hemm/metrics/vqa/judges/mmllm_judges/openai_judge.py</code> <pre><code>@weave.op()\ndef extract_prompt_parts(self, prompt: str) -&gt; List[TaggedPromptParts]:\n    \"\"\"Extract the prompt parts from the given prompt.\n\n    Args:\n        prompt (str): The prompt to extract the parts from.\n\n    Returns:\n        List[TaggedPromptParts]: List of tagged prompt objects.\n    \"\"\"\n    doc = self._nlp_pipeline(prompt)\n    tagged_prompt_parts: List[TaggedPromptParts] = []\n    for chunk in doc.noun_chunks:\n        chunk_np = chunk.text\n        for token in list(chunk)[::-1]:\n            if token.pos_ == \"NOUN\":\n                noun = token.lemma_\n                adjective = chunk_np.replace(f\" {noun}\", \"\")\n                adjective = adjective.replace(\"the \", \"\")\n                adjective = adjective.replace(\"a \", \"\")\n                adjective = adjective.replace(\"an \", \"\")\n                break\n        tagged_prompt_parts.append(\n            TaggedPromptParts(entity=chunk_np, noun=noun, adjective=adjective)\n        )\n    return tagged_prompt_parts\n</code></pre>"},{"location":"metrics/vqa/multi_modal_llm/#hemm.metrics.vqa.judges.mmllm_judges.openai_judge.OpenAIJudge.frame_question","title":"<code>frame_question(prompt, image)</code>","text":"<p>Frame the question corresponding to the given prompt and image for the chain-of-thought system of judgement.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to frame the question for.</p> required <code>image</code> <code>Image</code> <p>The image to frame the question for.</p> required <p>Returns:</p> Type Description <code>List[JudgeQuestion]</code> <p>List[JudgeQuestion]: List of questions to ask for the given prompt.</p> Source code in <code>hemm/metrics/vqa/judges/mmllm_judges/openai_judge.py</code> <pre><code>    @weave.op()\n    def frame_question(self, prompt: str, image: Image.Image) -&gt; List[JudgeQuestion]:\n        \"\"\"Frame the question corresponding to the given prompt and image for\n        the chain-of-thought system of judgement.\n\n        Args:\n            prompt (str): The prompt to frame the question for.\n            image (Image.Image): The image to frame the question for.\n\n        Returns:\n            List[JudgeQuestion]: List of questions to ask for the given prompt.\n        \"\"\"\n        if self.prompt_property in [PromptCategory.spatial, PromptCategory.spatial_3d]:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the objects and their spatial layout in the image.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to identify objects and their spatial layout in the image.\nYou have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate if the text \\\"{prompt}\\\" is correctly portrayed in the image.\nGive a score from 1 to 5, according to the following criteria:\n\n5: correct spatial layout in the image for all objects mentioned in the text.\n4: basically, spatial layout of objects matches the text.\n3: spatial layout not aligned properly with the text.\n2: image not aligned properly with the text.\n1: image almost irrelevant to the text.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        elif self.prompt_property == PromptCategory.action:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the the actions, events, objects and their relationships in the image.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to identify the actions, events, objects and their relationships in the image.\nYou have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate if the text \\\"{prompt}\\\" is correctly portrayed in the image.\nGive a score from 1 to 5, according to the following criteria:\n\n5: the image accurately portrayed the actions, events and relationships between objects described in the text.\n4: the image portrayed most of the actions, events and relationships but with minor discrepancies.\n3: the image depicted some elements, but action relationships between objects are not correct.\n2: the image failed to convey the full scope of the text.\n1: the image did not depict any actions or events that match the text.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        elif self.prompt_property == PromptCategory.numeracy:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the objects and their quantities in the image.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to identify objects and their quantities in the image.\nYou have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate how well the image aligns with the text prompt: \\\"{prompt}\\\"\nGive a score from 1 to 5, according to the following criteria:\n\n5: correct numerical content in the image for all objects mentioned in the text\n4: basically, numerical content of objects matches the text\n3: numerical content not aligned properly with the text\n2: image not aligned properly with the text\n1: image almost irrelevant to the text\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        elif self.prompt_property == PromptCategory.complex:\n            self._total_score = 5\n            question = JudgeQuestion(\n                image_desciption_system_prompt=\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to the objects in the image and their attributes\n(such as color, shape, texture), spatial layout and action relationships.\n                \"\"\",\n                judgement_question_system_prompt=\"\"\"\nYou are a helpful assistant meant to evaluate the correspondence of the image to a given text prompt.\nFocus on the objects in the image and their attributes (such as color, shape, texture),\nspatial layout and action relationships. You have to extract the question, the score, and the\nexplanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate how well the image aligns with the text prompt: \\\"{prompt}\\\"\nGive a score from 1 to 5, according to the following criteria:\n\n5: the image perfectly matches the content of the text prompt with no discrepancies.\n4: the image portrayed most of the actions, events and relationships but with minor discrepancies.\n3: the image depicted some elements in the text prompt, but ignored some key parts or details.\n2: the image did not depict any actions or events that match the text.\n1: the image failed to convey the full scope in the text prompt.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            return [(question, image)]\n        tagged_prompt_parts = self.extract_prompt_parts(prompt)\n        questions: List[str] = []\n        for tagged_prompt_part in tagged_prompt_parts:\n            question = JudgeQuestion(\n                image_desciption_system_prompt=f\"\"\"\nYou are a helpful assistant meant to describe images is detail.\nYou should pay special attention to any objects and their {self.prompt_property.name} in the given image.\n                \"\"\",\n                judgement_question_system_prompt=f\"\"\"\nYou are a helpful assistant meant to identify any objects and their {self.prompt_property.name}\nin the given image. You have to extract the question, the score, and the explanation from the user's response.\n                \"\"\",\n                judgement_question=f\"\"\"\nLooking at the image and given a detailed description of the image, evaluate if there is a {tagged_prompt_part.entity} in the image.\nGive a score from 1 to 4, according to the following criteria:\n\n4: there is {tagged_prompt_part.noun}, and {self.prompt_property.name} is {tagged_prompt_part.adjective}.\n3: there is {tagged_prompt_part.noun}, {self.prompt_property.name} is mostly {tagged_prompt_part.adjective}.\n2: there is {tagged_prompt_part.noun}, but it is not {tagged_prompt_part.adjective}.\n1: no {tagged_prompt_part.noun} in the image.\n\nHere are some more rules for scoring that you should follow:\n1. The shapes, layouts, orientations, and placements of the objects in the image should be realistic and adhere to physical constraints.\n    You should deduct 1 point from the score if there are any deformations with respect to the shapes, layouts, orientations, and\n    placements of the objects in the image.\n2. The anatomy of characters, humans, and animals should also be realistic and adhere to realistic constraints, shapes, and proportions.\n    You should deduct 1 point from the score if there are any deformations with respect to the anatomy of characters, humans, and animals\n    in the image.\n3. The spatial layout of the objects in the image should be consistent with the text prompt. You should deduct 1 point from the score if the\n    spatial layout of the objects in the image is not consistent with the text prompt.\n                \"\"\",\n            )\n            questions.append((question, image))\n        return questions\n</code></pre>"},{"location":"metrics/vqa/multi_modal_llm/#hemm.metrics.vqa.judges.mmllm_judges.openai_judge.OpenAIJudge.predict","title":"<code>predict(prompt, image)</code>","text":"<p>Predict the score for the given prompt and image.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to evaluate.</p> required <code>image</code> <code>Image</code> <p>The image to evaluate.</p> required Source code in <code>hemm/metrics/vqa/judges/mmllm_judges/openai_judge.py</code> <pre><code>@weave.op()\ndef predict(self, prompt: str, image: Image.Image) -&gt; List[OpenAIJudgeMent]:\n    \"\"\"Predict the score for the given prompt and image.\n\n    Args:\n        prompt (str): The prompt to evaluate.\n        image (Image.Image): The image to evaluate.\n    \"\"\"\n    questions = self.frame_question(prompt, image)\n    answers = []\n    for question, image in questions:\n        judgement_response: JudgeMent = self.execute_chain_of_thought(\n            question, image\n        )\n        judgement_response.explanation = (\n            f\"The score is {judgement_response.score}/{self._total_score}. \"\n            + judgement_response.explanation\n        )\n        answers.append(\n            OpenAIJudgeMent(\n                judgement=judgement_response,\n                fractional_score=judgement_response.score / self._total_score,\n            )\n        )\n    return answers\n</code></pre>"},{"location":"models/diffusion_model/","title":"Diffusion Models","text":""},{"location":"models/diffusion_model/#hemm.models.diffusion_model.BaseDiffusionModel","title":"<code>BaseDiffusionModel</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>weave.Model</code> wrapping <code>diffusers.DiffusionPipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion_model_name_or_path</code> <code>str</code> <p>The name or path of the diffusion model.</p> required <code>enable_cpu_offfload</code> <code>bool</code> <p>Enable CPU offload for the diffusion model.</p> <code>False</code> <code>image_height</code> <code>int</code> <p>The height of the generated image.</p> <code>512</code> <code>image_width</code> <code>int</code> <p>The width of the generated image.</p> <code>512</code> <code>num_inference_steps</code> <code>int</code> <p>The number of inference steps.</p> <code>50</code> <code>disable_safety_checker</code> <code>bool</code> <p>Disable safety checker for the diffusion model.</p> <code>True</code> <code>configs</code> <code>Dict[str, Any]</code> <p>Additional configs.</p> <code>{}</code> <code>pipeline_configs</code> <code>Dict[str, Any]</code> <p>Diffusion pipeline configs.</p> <code>{}</code> <code>inference_kwargs</code> <code>Dict[str, Any]</code> <p>Inference kwargs.</p> <code>{}</code> Source code in <code>hemm/models/diffusion_model.py</code> <pre><code>class BaseDiffusionModel(weave.Model):\n    \"\"\"`weave.Model` wrapping `diffusers.DiffusionPipeline`.\n\n    Args:\n        diffusion_model_name_or_path (str): The name or path of the diffusion model.\n        enable_cpu_offfload (bool): Enable CPU offload for the diffusion model.\n        image_height (int): The height of the generated image.\n        image_width (int): The width of the generated image.\n        num_inference_steps (int): The number of inference steps.\n        disable_safety_checker (bool): Disable safety checker for the diffusion model.\n        configs (Dict[str, Any]): Additional configs.\n        pipeline_configs (Dict[str, Any]): Diffusion pipeline configs.\n        inference_kwargs (Dict[str, Any]): Inference kwargs.\n    \"\"\"\n\n    diffusion_model_name_or_path: str\n    enable_cpu_offfload: bool = False\n    image_height: int = 512\n    image_width: int = 512\n    num_inference_steps: int = 50\n    disable_safety_checker: bool = True\n    configs: Dict[str, Any] = {}\n    pipeline_configs: Dict[str, Any] = {}\n    inference_kwargs: Dict[str, Any] = {}\n    _torch_dtype: torch.dtype = torch.float16\n    _pipeline: DiffusionPipeline = None\n\n    def __init__(\n        self,\n        diffusion_model_name_or_path: str,\n        enable_cpu_offfload: bool = False,\n        image_height: int = 512,\n        image_width: int = 512,\n        num_inference_steps: int = 50,\n        disable_safety_checker: bool = True,\n        configs: Dict[str, Any] = {},\n        pipeline_configs: Dict[str, Any] = {},\n        inference_kwargs: Dict[str, Any] = {},\n    ) -&gt; None:\n        super().__init__(\n            diffusion_model_name_or_path=diffusion_model_name_or_path,\n            enable_cpu_offfload=enable_cpu_offfload,\n            image_height=image_height,\n            image_width=image_width,\n            num_inference_steps=num_inference_steps,\n            disable_safety_checker=disable_safety_checker,\n            configs=configs,\n            pipeline_configs=pipeline_configs,\n            inference_kwargs=inference_kwargs,\n        )\n        self.configs[\"torch_dtype\"] = str(self._torch_dtype)\n        pipeline_init_kwargs = {\n            \"pretrained_model_name_or_path\": self.diffusion_model_name_or_path,\n            \"torch_dtype\": self._torch_dtype,\n        }\n        pipeline_init_kwargs.update(self.pipeline_configs)\n        if self.disable_safety_checker:\n            pipeline_init_kwargs[\"safety_checker\"] = None\n        self._pipeline = DiffusionPipeline.from_pretrained(**pipeline_init_kwargs)\n        if self.enable_cpu_offfload:\n            self._pipeline.enable_model_cpu_offload()\n        else:\n            self._pipeline = self._pipeline.to(\"cuda\")\n        self._pipeline.set_progress_bar_config(leave=False, desc=\"Generating Image\")\n\n    @weave.op()\n    def predict(self, prompt: str, seed: int) -&gt; Dict[str, Any]:\n        pipeline_output = self._pipeline(\n            prompt,\n            num_images_per_prompt=1,\n            height=self.image_height,\n            width=self.image_width,\n            generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n            num_inference_steps=self.num_inference_steps,\n            **self.inference_kwargs,\n        )\n        return {\"image\": pipeline_output.images[0]}\n</code></pre>"},{"location":"models/falai_model/","title":"FalAI Models","text":""},{"location":"models/falai_model/#hemm.models.falai_model.FalAIModel","title":"<code>FalAIModel</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>weave.Model</code> wrapping FalAI calls.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>FalAI model name.</p> required <code>inference_kwargs</code> <code>Dict[str, Any]</code> <p>Inference kwargs.</p> required Source code in <code>hemm/models/falai_model.py</code> <pre><code>class FalAIModel(weave.Model):\n    \"\"\"`weave.Model` wrapping [FalAI](https://fal.ai/) calls.\n\n    Args:\n        model_name (str): FalAI model name.\n        inference_kwargs (Dict[str, Any]): Inference kwargs.\n    \"\"\"\n\n    model_name: str\n    inference_kwargs: Dict[str, Any] = {}\n\n    @weave.op()\n    def generate_image(self, prompt: str, seed: int) -&gt; Image.Image:\n        result = custom_weave_wrapper(name=\"fal_client.submit.get\")(\n            fal_client.submit(\n                self.model_name,\n                arguments={\"prompt\": prompt, \"seed\": seed, **self.inference_kwargs},\n            ).get\n        )()\n        return load_image(result[\"images\"][0][\"url\"])\n\n    @weave.op()\n    def predict(self, prompt: str, seed: int) -&gt; Image.Image:\n        return {\"image\": self.generate_image(prompt=prompt, seed=seed)}\n</code></pre>"},{"location":"models/stability_model/","title":"StabilityAI Model","text":""},{"location":"models/stability_model/#hemm.models.stability_model.StabilityAPIModel","title":"<code>StabilityAPIModel</code>","text":"<p>               Bases: <code>Model</code></p> <p><code>weave.Model</code> wrapping Stability API calls.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Stability model name.</p> required <code>aspect_ratio</code> <code>str</code> <p>Aspect ratio of the generated image.</p> <code>'1:1'</code> <code>creativity</code> <code>float</code> <p>Creativity of the generated image.</p> <code>0.35</code> Source code in <code>hemm/models/stability_model.py</code> <pre><code>class StabilityAPIModel(weave.Model):\n    \"\"\"`weave.Model` wrapping Stability API calls.\n\n    Args:\n        model_name (str): Stability model name.\n        aspect_ratio (str): Aspect ratio of the generated image.\n        creativity (float): Creativity of the generated image.\n    \"\"\"\n\n    model_name: str\n    aspect_ratio: str = \"1:1\"\n    creativity: float = 0.35\n    configs: Dict[str, Any] = {}\n\n    def __init__(\n        self,\n        model_name: str,\n        aspect_ratio: str = \"1:1\",\n        creativity: float = 0.35,\n    ) -&gt; None:\n        assert aspect_ratio in [\n            \"1:1\",\n            \"16:9\",\n            \"21:9\",\n            \"2:3\",\n            \"3:2\",\n            \"4:5\",\n            \"5:4\",\n            \"9:16\",\n            \"9:21\",\n        ], \"Invalid aspect ratio\"\n        super().__init__(\n            model_name=model_name, aspect_ratio=aspect_ratio, creativity=creativity\n        )\n\n    @weave.op()\n    def send_generation_request(self, prompt: str, seed: int):\n        api_key = os.environ[\"STABILITY_KEY\"]\n        headers = {\"Accept\": \"image/*\", \"Authorization\": f\"Bearer {api_key}\"}\n        response = requests.post(\n            STABILITY_MODEL_HOST[self.model_name],\n            headers=headers,\n            files={\"none\": \"\"},\n            data={\n                \"prompt\": prompt,\n                \"negative_prompt\": \"\",\n                \"aspect_ratio\": self.aspect_ratio,\n                \"seed\": seed,\n                \"output_format\": \"png\",\n                \"model\": self.model_name,\n                \"mode\": \"text-to-image\",\n                \"creativity\": self.creativity,\n            },\n        )\n        if not response.ok:\n            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n        return response\n\n    @weave.op()\n    def predict(self, prompt: str, seed: int) -&gt; Image.Image:\n        response = self.send_generation_request(prompt=prompt, seed=seed)\n        image = Image.open(io.BytesIO(response.content))\n        return {\"image\": image}\n</code></pre>"}]}